\chapter{Finite differences in 2D}\label{chap: finite diff 2d}

In this chapter, we study the numerical solution of the Dirichlet boundary-value 
problem for the Poisson equation. Let $\Omega$ be a bounded, open subset 
of~$\mathbb{R}^2$, with a piecewise smooth boundary~$\Gamma=\partial\Omega$.  
Given suitable functions $f(x,y)$~and $g(x,y)$, we see $u=u(x,y)$ satisfying
\begin{equation}\label{eq: Poisson bvp}
\begin{aligned}
-\nabla^2u&=f(x,y)&&\text{for $(x,y)\in\Omega$,}\\
u&=g(x,y)&&\text{for $(x,y)\in\Gamma$.}
\end{aligned}
\end{equation}
Here, the \emph{Laplacian} is the second-order elliptic differential operator
defined by
\[
\nabla^2 u=\nabla\cdot(\nabla u)=\frac{\partial^2u}{\partial x^2}
    +\frac{\partial^2u}{\partial y^2}.
\]

\section{Five-point difference scheme}\label{sec: five point scheme}
For simplicity, we now restrict our attention to the case when the spatial 
domain is a rectangle,
\begin{equation}\label{eq: Omega rectangle}
\Omega=(0,L_x)\times(0,L_y).
\end{equation}
To set up the spatial finite difference grid, we fix positive integers $P$~and 
$Q$, define the step sizes
\[
\Delta x=\frac{L_x}{P}\quad\text{and}\quad\Delta y=\frac{L_y}{Q},
\]
and define the grid points
\begin{equation}\label{eq: xp yq grid}
(x_p,y_q)=(p\,\Delta x,q\,\Delta y)
\quad\text{for $0\le p\le P$ and $0\le q\le Q$.}
\end{equation}
Our is to compute $U_{p,q}\approx u(x_p,y_q)$ where $u$ is the solution 
of~\eqref{eq: Poisson bvp}.

Let $\delta_x^2$~and $\delta_y^2$ denote the second-order, central difference 
operators in the $x$- and $y$-directions, respectively; that is,
\[
\delta_x^2u(x,y)=\frac{u(x+\Delta x,y)-2u(x,y)+u(x-\Delta x,y)}{\Delta x^2}
    =\frac{\partial^2u}{\partial x^2}+O(\Delta x^2)
\]
and
\[
\delta_y^2u(x,y)=\frac{u(x,y+\Delta y)-2u(x,y)+u(x,y-\Delta y)}{\Delta x^2}
    =\frac{\partial^2u}{\partial y^2}+O(\Delta y^2).
\]
We also write
\[
\delta_x^2U_{pq}=\frac{U_{p+1,q}-2U_{p,q}+U_{p-1,q}}{\Delta x^2}
\quad\text{and}\quad
\delta_y^2U_{pq}=\frac{U_{p,q}-2U_{p,q}+U_{p,q-1}}{\Delta y^2}.
\]
With this notation, our finite difference scheme can be written compactly as
\begin{equation}\label{eq: 5-pt Poisson}
\begin{aligned}
-\bigl(\delta_x^2U_{p,q}+\delta_y^2U_{p,q}\bigr)&=f_{p,q}
    &&\text{for $(x_p,y_q)\in\Omega$,}\\
U_{p,q}&=g_{p,q}&&\text{for $(x_p,y_q)\in\Gamma$,}
\end{aligned}
\end{equation}
with the obvious abbreviations $f_{p,q}=f(x_p,y_q)$~and $g_{p,q}=g(x_p,y_q)$.
Notice that $(x_p,y_q)\in\Omega$ for $1\le p\le P-1$ and $1\le q\le Q-1$, so 
there are 
\[
M=(P-1)(Q-1)
\]
unknown values of~$U_{p,q}$ at the interior grid points.  The remaining
$(P+1)(Q+1)-M=2P+2Q$ values are given directly by the Dirichlet boundary 
condition.  The finite difference approximation provides one equation for 
each interior grid points, and hence one equation for each unknown, to yield an 
$M\times M$~linear system.  Figure~\ref{fig: 5-pt stencil} shows the stencil 
for the scheme, which involes 5~grid points: $(x_p,y_q)$ and its four nearest 
neighbours $(x_{p-1},y_q)$, $(x_{p+1},y_q)$, $(x_{p,q-1})$~and 
$(x_p,y_{q+1})$.

\begin{figure}
\caption{Five-point finite difference stencil for the discrete Poisson 
equation~\eqref{eq: 5-pt Poisson}.}\label{fig: 5-pt stencil}
\begin{center}
\begin{tikzpicture}[scale=0.5]
\draw[->] (-1,0) -- (17,0);
\node[right] at (17,0) {$x$};
\node[below] at (8,0) {$x_p$};
\draw[->] (0,-1) -- (0,13);
\node[above left] at (0,13) {$y$};
\node[left] at (0,6) {$y_q$};
\foreach \x in {2, 4, ..., 14}
    \draw[thin] (\x,0) -- (\x,12);
\foreach \y in {2, 4, ..., 10}
    \draw[thin] (0,\y) -- (16,\y);
\draw[ultra thick] (8,4) -- (8,8);
\draw[ultra thick] (6,6) -- (10,6);
\draw[fill=red] (8,4)  circle (0.15cm);
\draw[fill=red] (8,6)  circle (0.15cm);
\draw[fill=red] (8,8)  circle (0.15cm);
\draw[fill=red] (6,6) circle (0.15cm);
\draw[fill=red] (10,6) circle (0.15cm);
\node[below] at (16,0) {$L_x$};
\node[left]  at (0,12) {$L_y$};
\draw[thick] (0,0) -- (16,0) -- (16,12) -- (0,12) -- (0,0);
\end{tikzpicture}
\end{center}
\end{figure}

To describe the $M\times M$ system explicitly, we need to arrange the 
unknowns~$U_{p,q}$ into a column vector of length~$M$.  A standard approach is 
to think of the $U_{p,q}$ as the entries of a $(P-1)\times(Q-1)$ matrix and use 
\emph{column-major ordering}, so that
\begin{equation}\label{eq: column-major}
U_j=U_{p,q}\quad
\text{where $j=p+(q-1)(P-1)$ for $1\le p\le P-1$ and $1\le q\le Q-1$.}
\end{equation}
The right-hand sides $f_{p,q}$ are arranged in the same way.  

\begin{example}\label{example: 5-pt matrix}
Suppose $P=5$ and $Q=4$, with $\Delta x=h=\Delta y$.  The finite difference 
equation is then
\[
-\frac{1}{h^2}\bigl(U_{p+1,q}-2U_{p,q}+U_{p-1,q}+U_{p,q+1}2-U_{p,q}+U_{p,q-1}
    \bigr)=f_{p,q},
\]
or equivalently,
\[
\frac{1}{h^2}\bigl(-U_{p,q-1}-U_{p-1,q}+4U_{p,q}-U_{p+1,q}-U_{p,q+1}\bigr)
    =f_{p,q}.
\]
When $p=1$~and $q=3$,
\[
\frac{1}{h^2}\bigl(-U_{1,2}-U_{0,3}+4U_{1,3}-U_{2,3}-U_{1,4}\bigr)
    =f_{1,3},
\]
and the boundary conditions give $U_{0,3}=g_{0,3}$~and $U_{1,4}=g_{1,4}$, so
\[
\frac{1}{h^2}\bigl(-U_{1,2}+4U_{1,3}-U_{2,3}\bigr)
    =f_{1,3}+\frac{1}{h^2}\bigl(g_{0,3}+g_{1,4}\bigr).
\]
Using column-major ordering~\eqref{eq: column-major} with~$P=5$, we have
$U_5=U_{1,2}$, $U_9=U_{1,3}$~and $U_{10}=U_{2,3}$ so the $9$th equation is
\[
\frac{1}{h^2}\bigl(-U_5+4U_9-U_{10}\bigr)=f_9
    +\frac{1}{h^2}\bigl(g_{0,3}+g_{1,4}\bigr).
\]
Figure~\ref{fig: 5-pt matrix} shows the complete $12\times12$ linear system.
\end{example}

\begin{figure}
\caption{The $12\times12$ linear system from 
Example~\ref{example: 5-pt matrix}.}\label{fig: 5-pt matrix}
\begin{gather*}
\frac{1}{h^2}
\left[\begin{array}{cccc|cccc|cccc}
 4&-1& 0& 0&  -1& 0& 0& 0&   0& 0& 0& 0\\
-1& 4&-1& 0&   0&-1& 0& 0&   0& 0& 0& 0\\
 0&-1& 4&-1&   0& 0&-1& 0&   0& 0& 0& 0\\
 0& 0&-1& 4&   0& 0& 0&-1&   0& 0& 0& 0\\
\hline
-1& 0& 0& 0&   4&-1& 0& 0&  -1& 0& 0& 0\\
 0&-1& 0& 0&  -1& 4&-1& 0&   0&-1& 0& 0\\
 0& 0&-1& 0&   0&-1& 4&-1&   0& 0&-1& 0\\
 0& 0& 0&-1&   0& 0&-1& 4&   0& 0& 0&-1\\
\hline
 0& 0& 0& 0&  -1& 0& 0& 0&   4&-1& 0& 0\\
 0& 0& 0& 0&   0&-1& 0& 0&  -1& 4&-1& 0\\
 0& 0& 0& 0&   0& 0&-1& 0&   0&-1& 4&-1\\
 0& 0& 0& 0&   0& 0& 0&-1&   0& 0&-1& 4
\end{array}\right]
\left[\begin{array}{c}
U_{1,1}\\ U_{2,1}\\ U_{3,1}\\ U_{4,1}\\ 
\hline
U_{1,2}\\ U_{2,2}\\ U_{3,2}\\ U_{4,2}\\                
\hline
U_{1,3}\\ U_{2,3}\\ U_{3,3}\\ U_{4,3}                
\end{array}\right]\\
=\left[\begin{array}{c}
f_{1,1}\\ f_{2,1}\\ f_{3,1}\\ f_{4,1}\\                
\hline
f_{1,2}\\ f_{2,2}\\ f_{3,2}\\ f_{4,2}\\                
\hline
f_{1,3}\\ f_{2,3}\\ f_{3,3}\\ f_{4,3}                
\end{array}\right]
+\frac{1}{h^2}\left[\begin{array}{c}
g_{01}+g_{10}\\ g_{20}\\ g_{30}\\ g_{40}+g_{51}\\ \hline
g_{02}       \\ 0     \\ 0     \\ g_{52}\\ \hline
g_{03}+g_{14}\\ g_{24}\\ g_{34}\\ g_{44}+g_{53}
\end{array}\right].
\end{gather*}
\end{figure}

\section{Matrix structure}
The structure of the matrix arising from the discrete Poisson 
problem~\eqref{eq: 5-pt Poisson} can be understood more easily with the help of 
the following concept.

\begin{definition}
Given matrices
$\boldsymbol{A}\in\mathbb{R}^{M\times N}$~and 
$\boldsymbol{B}\in\mathbb{R}^{P\times Q}$, the \emph{Kronecker product} 
$\boldsymbol{A}\otimes\boldsymbol{B}\in\mathbb{R}^{(MP)\times(NQ)}$ 
is the $M\times N$ block matrix whose $ij$-block equals $a_{ji}\boldsymbol{B}$,
that is,
\[
\boldsymbol{A}\otimes\boldsymbol{B}=\begin{bmatrix}
a_{11}\boldsymbol{B}&a_{12}\boldsymbol{B}&\cdots&a_{1N}\boldsymbol{B}\\
a_{21}\boldsymbol{B}&a_{22}\boldsymbol{B}&\cdots&a_{2N}\boldsymbol{B}\\
              \vdots&              \vdots&\ddots&              \vdots\\
a_{M1}\boldsymbol{B}&a_{M2}\boldsymbol{B}&\cdots&a_{MN}\boldsymbol{B}
\end{bmatrix}.
\]
\end{definition}

\begin{example}\label{example: Kronecker product}
If
\[
\boldsymbol{A}=\begin{bmatrix}1&3\\ 2&4\end{bmatrix}
\quad\text{and}\quad
\boldsymbol{B}=\begin{bmatrix}1&0&-3\\ -2&1&-1\end{bmatrix}
\]
then
\[
\boldsymbol{A}\otimes\boldsymbol{B}
=\begin{bmatrix}
 \boldsymbol{B}&3\boldsymbol{B}\\
2\boldsymbol{B}&4\boldsymbol{B} 
\end{bmatrix}
=\left[\begin{array}{ccc|ccc}
 1& 0&-3& 3& 0&-9\\
-2& 1&-1&-6& 3&-3\\             
\hline
 2& 0&-6& 4& 0&-12\\
-4& 2&-2&-8& 4&-4
\end{array}\right].
\]
\end{example}

The next theorem gives a key relation between the Kronecker product and the 
ordinary matrix product.

\begin{theorem}\label{thm: Kronecker A B C D}
If $\boldsymbol{A}\in\mathbb{R}^{M\times N}$,
$\boldsymbol{B}\in\mathbb{R}^{P\times Q}$,
$\boldsymbol{C}\in\mathbb{R}^{N\times R}$~and
$\boldsymbol{D}\in\mathbb{R}^{Q\times S}$, then
\[
(\boldsymbol{A}\otimes\boldsymbol{B})(\boldsymbol{C}\otimes\boldsymbol{D})
=(\boldsymbol{A}\boldsymbol{C})\otimes(\boldsymbol{B}\boldsymbol{D}).
\]
\end{theorem}
\begin{proof}
The $ij$-block of 
$(\boldsymbol{A}\otimes\boldsymbol{B})(\boldsymbol{C}\otimes\boldsymbol{D})$ 
equals
\[
\sum_{k=1}^N(a_{ik}\boldsymbol{B})(c_{kj}\boldsymbol{D})
    =\biggl(\sum_{k=1}^Na_{ik}c_{kj}\biggr)(\boldsymbol{B}\boldsymbol{D})
    =(\boldsymbol{A}\boldsymbol{C})_{ij}(\boldsymbol{B}\boldsymbol{D}),
\]
which is also the $ij$-block of
$(\boldsymbol{A}\boldsymbol{C})\otimes(\boldsymbol{B}\boldsymbol{D})$.
\end{proof}

\begin{example}
\newcommand{\bs}[1]{\boldsymbol{#1}}
With $\boldsymbol{A}$~and $\boldsymbol{B}$ as in 
Example~\ref{example: Kronecker product}, and with
\[
\boldsymbol{C}=\begin{bmatrix}1&-1&3\\ 5&0&3\end{bmatrix}
\quad\text{and}\quad
\boldsymbol{D}=\begin{bmatrix}1&0&7\\ 0&-4&1\\ 2&3&9\end{bmatrix},
\]
we have
\[
(\bs{A}\otimes\bs{B})(\bs{C}\otimes\bs{D})
    =\begin{bmatrix}
 \bs{B}&3\bs{B}\\
2\bs{B}&4\bs{B}\end{bmatrix}
\begin{bmatrix}
 \bs{D}&-\bs{D}&3\bs{D}\\
5\bs{D}& \bs{0}&3\bs{D}
\end{bmatrix}
=\begin{bmatrix}
16\bs{B}\bs{D}& -\bs{B}\bs{D}&12\bs{B}\bs{D}\\
22\bs{B}\bs{D}&-2\bs{B}\bs{D}&18\bs{B}\bs{D}
\end{bmatrix}
\]
which is the same matrix as
\[
(\bs{A}\bs{C})\otimes(\bs{B}\bs{D})
=\left(\begin{bmatrix}1&3\\ 2&4\end{bmatrix}
\begin{bmatrix}1&-1&3\\ 5&0&3\end{bmatrix}\right)\otimes(\bs{B}\bs{D})
=\begin{bmatrix}16&-1&12\\ 22&-2&18\end{bmatrix}\otimes(\bs{B}\bs{D}).
\]
\end{example}

The Kronecker product makes sense for vector operands, if we identify an
$N$-dimensional row vector with a $1\times N$~matrix, and identify an 
$N$-dimensional column vector with an $N\times1$~matrix.  Suppose that
\[
\newcommand{\bs}[1]{\boldsymbol{#1}}
\bs{U}=\bs{w}\otimes\bs{v}
    =\begin{bmatrix}w_1\bs{v}\\ w_2\bs{v}\\ \vdots\\ w_{Q-1}\bs{v}\end{bmatrix}
\quad\text{for $\bs{v}\in\mathbb{R}^{P-1}$ and $\bs{w}\in\mathbb{R}^{Q-1}$,}
\]
so that
\[
U_j=U_{p,q}=v_pw_q\quad\text{where $j=p+(q-1)(P-1)$,}
\]
for $1\le p\le P-1$ and $1\le q\le Q-1$.  Define
\[
\boldsymbol{A}_x=\frac{1}{\Delta x^2}\begin{bmatrix}
 2&    -1&      &      &\\
-1&     2&    -1&      &\\
  &\ddots&\ddots&\ddots&\\
  &      &    -1&     2&-1\\
  &      &      &    -1& 2\end{bmatrix}\in\mathbb{R}^{(P-1)\times(P-1)}
\]
and
\[
\boldsymbol{A}_y=\frac{1}{\Delta y^2}\begin{bmatrix}
 2&    -1&      &      &\\
-1&     2&    -1&      &\\
  &\ddots&\ddots&\ddots&\\
  &      &    -1&     2&-1\\
  &      &      &    -1& 2\end{bmatrix}\in\mathbb{R}^{(Q-1)\times(Q-1)},
\]
so that
\begin{equation}\label{eq: delta x y separable}
-\delta_x^2U_{p,q}=(\boldsymbol{A}_x\boldsymbol{v})_pw_q
\quad\text{and}\quad
-\delta_y^2U_{p,q}=v_p(\boldsymbol{A}_y\boldsymbol{w})_q,
\end{equation}
if we set $v_0=0=v_P$ and $w_0=0=w_Q$.  These relations correspond to the case 
when $u(x,y)=v(x)w(y)$ so that $-u_{xx}=(-v'')w$~and $u_{yy}=v(-w'')$.

\begin{theorem}\label{thm: Poisson matrix}
Let
\[
\boldsymbol{A}\boldsymbol{U}=\boldsymbol{f}+\boldsymbol{g}
\]
be the linear system resulting from the discrete Poisson 
problem~\eqref{eq: 5-pt Poisson} scheme~\eqref{eq: 5-pt Poisson}.  Then,
\[
\boldsymbol{A}=\boldsymbol{I}_y\otimes\boldsymbol{A}_x
    +\boldsymbol{A}_y\otimes\boldsymbol{I}_x,
\]
where $\boldsymbol{I}_x$~and $\boldsymbol{I}_y$ are the identity matrices of 
dimension $P-1$~and $Q-1$, respectively.
\end{theorem}
\begin{proof}
We see from \eqref{eq: delta x y separable}~and 
Theorem~\ref{thm: Kronecker A B C D} that
\begin{align*}
\boldsymbol{A}(\boldsymbol{w}\otimes\boldsymbol{v})
    &=\boldsymbol{w}\otimes(\boldsymbol{A}_x\boldsymbol{v})
    +(\boldsymbol{A}_y\boldsymbol{w})\otimes\boldsymbol{v}\\
    &=\bigl(\boldsymbol{I}_y\boldsymbol{w})
        \otimes(\boldsymbol{A}_x\boldsymbol{v})
    +(\boldsymbol{A}_y\boldsymbol{w})
        \otimes(\boldsymbol{I}_x\boldsymbol{v})\\
    &=(\boldsymbol{I}_y\otimes\boldsymbol{A}_x)
        (\boldsymbol{w}\otimes\boldsymbol{v})
    +(\boldsymbol{A}_y\otimes\boldsymbol{I}_x)
        (\boldsymbol{w}\otimes\boldsymbol{v})\\
    &=\bigl(\boldsymbol{I}_y\otimes\boldsymbol{A}_x
    +\boldsymbol{A}_y\otimes\boldsymbol{I}_x\bigr)
        (\boldsymbol{w}\otimes\boldsymbol{v})
\end{align*}
for every $\boldsymbol{v}\in\mathbb{R}^{P-1}$ and 
$\boldsymbol{w}\in\mathbb{R}^{Q-1}$.
\end{proof}


\section{Band Cholesky factorization}

A matrix~$\boldsymbol{A}=[a_{ij}]$ has \emph{upper bandwidth}~$\beta$ 
if $a_{ij}=0$ whenever $j-i>\beta$, or equivalently if $\boldsymbol{A}$ has 
$\beta$~non-zero superdiagonals.  Similarly, $\boldsymbol{A}$ has \emph{lower 
bandwidth}~$\beta$ if $a_{ij}=0$ whenever $i-j>\beta$, so that there are 
$\beta$~non-zero subdiagonals.  For example, the following matrix has upper 
bandwidth~$2$ and lower bandwidth~$3$:
\[
\begin{bmatrix}
 5& 2&-1& 0& 0& 0& 0\\
 1& 6& 0& 8& 0& 0& 0\\
 2& 3& 9& 1& 8& 0& 0\\
-3& 4& 7& 2& 5& 8& 0\\
 0& 0& 2& 8& 3&-7& 1\\
 0& 0& 4&-5& 6& 9& 3
\end{bmatrix}.
\]
If a matrix is symmetric, then its upper and lower bandwidths must be equal, so 
we refer to both as just the \emph{bandwidth}.

Let $\boldsymbol{A}$ be an $n\times n$ symmetric, positive-definite matrix
with bandwith~$\beta$.  We will seek a \emph{band Cholesky factorization}
\begin{equation}\label{eq: Chol fact}
\boldsymbol{A}=\boldsymbol{R}^T\boldsymbol{R},
\end{equation}
where the $n\times n$ matrix~$\boldsymbol{R}=[r_{ij}]$ is upper triangular with 
upper bandwidth~$\beta$.  By the definition of matrix multiplication,
\[
\bigl(\boldsymbol{R}^T\boldsymbol{R}\bigr)_{ij}
	=\sum_{k=1}^n\bigl(\boldsymbol{R}^T\bigr)_{ik}\boldsymbol{R}_{kj}
	=\sum_{k=1}^n r_{ki}r_{kj}.
\]
Since $\boldsymbol{R}$ is upper triangular, $r_{ki}r_{kj}=0$ if $k>i$~or $k>j$,
that is, if $k>\min(i,j)$, so in fact
\[
\bigl(\boldsymbol{R}^T\boldsymbol{R}\bigr)_{ij}
	=\sum_{k=1}^{\min(i,j)} r_{ki}r_{kj}.
\]
In addition, $\boldsymbol{R}$ has upper bandwidth~$\beta$ so $r_{ki}r_{kj}=0$ 
if $i>k+\beta$~or $j>k+\beta$.  Thus, taking account of symmetry, 
\eqref{eq: Chol fact} is satisfied iff
\[
a_{ij}=\sum_{k=\max(1,j-\beta)}^ir_{ki}r_{kj}
	\quad\text{for $\max(1,j-\beta)\le i\le j\le n$.}
\]
We split the last term off the sum, writing
\[
a_{ij}=r_{ii}r_{ij}+\sum_{k=\max(1,j-\beta)}^{i-1}r_{ki}r_{kj}
	\quad\text{for $\max(1,j-\beta)\le i<j\le n$,}
\]
in the off-diagonal case, and
\[
a_{jj}=r_{jj}^2+\sum_{k=\max(1,j-\beta)}^{j-1}r_{kj}^2
	\quad\text{for $1\le j\le n$.}
\]
in the diagonal case.  Rearranging these equations leads to the formulae
\[
r_{ij}=\frac{1}{r_{ii}}\biggl(a_{ij}-\sum_{k=\max(1,j-\beta)}^{i-1}r_{ki}r_{kj}
	\biggr)\quad\text{for $\max(1,j-\beta)\le i\le j\le n$,}
\]
and
\[
r_{jj}=\sqrt{a_{jj}-\sum_{k=\max(1,j-\beta)}^{j-1}r_{kj}^2}
	\quad\text{for $1\le j\le n$,}
\]
which yield Algorithm~\ref{alg: band Chol}.

\begin{algorithm}
G\caption{Compute the band Cholesky factorization \eqref{eq: Chol fact}.}
\label{alg: band Chol}
\begin{algorithmic}
\Require{$\boldsymbol{A}=[a_{ij}]$ is a real, $n\times n$, symmetric 
positive-definite matrix with bandwidth~$\beta$.}
\State
\Function{Factorize}{$\boldsymbol{A}$}
\State Allocate storage for the $n\times n$ Cholesky 
factor~$\boldsymbol{R}=[r_{ij}]$ and initialize to zero.
\For{$j=1:n$}
    \For{$i=\max(1,j-\beta):j$}
        \State $s=0$
        \For{$k=\max(1,j-\beta):i-1$}
            \State $s=s+r_{ki}r_{kj}$
        \EndFor
        \State $r_{ij}=(a_{ij}-s)/r_{ii}$
    \EndFor
    \State $s=0$
    \For{$k=\max(1,j-\beta):j-1$}
        \State $s=s+r_{kj}^2$
    \EndFor
    \If{$a_{jj}\le s$} 
        \State Error: $\boldsymbol{A}$ is not positive-definite.
    \EndIf
    \State $r_{jj}=\sqrt{a_{jj}-s}$
\EndFor
\State\Return{$\boldsymbol{R}$}
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Solve the linear system $\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}$ 
given the band Cholesky factorization~\eqref{eq: Chol fact}.}
\label{alg: band Chol solve}
\begin{algorithmic}
\Require{$\boldsymbol{b}=[b_i]_{i=1}^n$ is the right-hand side vector.}
\Require{$\boldsymbol{R}=[r_{ij}]_{i,j=1}^n$ is the band Cholesky factor 
of~$\boldsymbol{A}$, computed via Algorithm~\ref{alg: band Chol}.}
\State
\Function{Solve}{$\boldsymbol{b}, \boldsymbol{R}$}
\State Allocate storage for $\boldsymbol{x}=[x_i]_{i=1}^n$~and
$\boldsymbol{y}=[y_i]_{i=1}^n$.
\For{$i=1:n$}
    \State $s=b_i$
    \For{$k=\max(1,i-\beta):i-1$}
        \State $s=s-r_{ji}y_j$
    \EndFor
    \State $y_i=s/r_{ii}$
\EndFor
\For{$i=n:-1:1$}
    \State $s=y_i$
    \For{$j=i+1:\min(n,i+\beta)$}
        \State $s=s-r_{ij}x_j$
    \EndFor
    \State $x_i=s/r_{ii}$
\EndFor
\State\Return{$\boldsymbol{x}$}
\EndFunction
\end{algorithmic}
\end{algorithm}

Once the Cholesky factor~$\boldsymbol{R}$ is known, we can solve a linear 
system~$\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}$ by first solving the lower 
triangular system $\boldsymbol{R}^T\boldsymbol{y}=\boldsymbol{b}$, and then 
solving the upper triangular 
system~$\boldsymbol{R}\boldsymbol{x}=\boldsymbol{y}$, so that
\[
\boldsymbol{A}\boldsymbol{x}=\boldsymbol{R}^T\boldsymbol{R}\boldsymbol{x}
	=\boldsymbol{R}^T\boldsymbol{y}=\boldsymbol{b}.
\]
Since $r_{ji}=0$ if $j>i$~or $i-j>s$,
\[
(\boldsymbol{R}^T\boldsymbol{y})_i=\sum_{j=1}^nr_{ji}y_j
	=\sum_{j=\max(1,i-\beta)}^ir_{ji}y_j
	=r_{ii}y_i+\sum_{j=\max(1,i-\beta)}^{i-1}r_{ji}y_j
\]
and so $\boldsymbol{R}^T\boldsymbol{y}=\boldsymbol{b}$ iff
\[
y_i=\frac{1}{r_{ii}}\biggl(b_i-\sum_{j=\max(1,i-\beta)}^{i-1}r_{ji}y_j\biggr)
\quad\text{for $1\le i\le n$.}
\]
Similarly, since $r_{ij}=0$ if $i>j$~or $j-i>\beta$,
\[
(\boldsymbol{R}\boldsymbol{x})_i=\sum_{j=1}^nr_{ij}x_j
	=\sum_{j=i}^{\min(n,i+\beta)}r_{ij}x_j
	=r_{ii}x_i+\sum_{j=i+1}^{\min(n,i+\beta)}r_{ij}x_j
\]
and so $\boldsymbol{R}\boldsymbol{x}=\boldsymbol{y}$ iff
\[
x_i=\frac{1}{r_{ii}}\biggl(y_i-\sum_{j=i+1}^{\min(n,i+\beta)}r_{ij}x_j\biggr)
	\quad\text{for $1\le i\le n$.}
\]
These formula lead to Algorithm~\ref{alg: band Chol solve} for solving 
$\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}$.

\begin{algorithm}
\newcommand{\rb}[1]{r^{\mathrm{band}}_{#1}}
\caption{Solve in place the linear system 
$\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}$ 
given the band Cholesky factorization~\eqref{eq: Chol fact}.}
\label{alg: band Chol solve in place}
\begin{algorithmic}
\Require{$\boldsymbol{x}=[b_i]_{i=1}^n$ stores the right-hand side vector.}
\Require{The $(\beta+1)\times n$ array 
$\boldsymbol{R}_{\mathrm{band}}=[\rb{ij}]$ stores 
the band Cholesky factor~$\boldsymbol{R}=[r_{ij}]$ of~$\boldsymbol{A}$, as 
computed via Algorithm~\ref{alg: band Chol} so that 
$r_{ij}=\rb{\beta+1+i-j,j}$.}
\State
\Function{Solve}{$\boldsymbol{x},\boldsymbol{R}_{\mathrm{band}}$}
\For{$i=1:n$}
    \State $s=x_i$
    \For{$j=\max(1,i-\beta):i-1$}
        \State $s=s-\rb{\beta+1+j-i,i}x_j$
    \EndFor
    \State $x_i=s/\rb{\beta+1,i}$
\EndFor
\For{$i=n:-1:1$}
    \State $s=x_i$
    \For{$j=i+1:\min(n,i+\beta)$}
        \State $s=s-\rb{\beta+1+i-j,j}x_j$
    \EndFor
    \State $x_i=s/\rb{\beta+1,i}$
\EndFor
\State\Return{$\boldsymbol{x}$}
\EndFunction
\end{algorithmic}
\end{algorithm}

When $\beta$ is small compared to~$n$, storing $\boldsymbol{A}$ as an 
$n\times n$~array wastes a lot of space.  A more efficient method is to use 
the \emph{symmetric band storage scheme} that uses a
$(\beta+1)\times n$~matrix 
$\boldsymbol{A}_{\textrm{band}}=[a^{\mathrm{band}}_{ij}]$ to hold the non-zero 
superdiagonals of~$\boldsymbol{A}$ by putting
\begin{equation}\label{eq: symm band storage}
a^{\mathrm{band}}_{\beta+1+i-j,j}=a_{ij}
	\quad\text{for $\max(1,j-\beta)\le i\le j$ and $1\le j\le n$.}
\end{equation}
For example, if $n=9$ and $\beta=3$, then
\[
\boldsymbol{A}_{\mathrm{b}}=\begin{bmatrix}
     *&     *&     *&a_{14}&a_{25}&a_{36}&a_{47}&a_{58}&a_{69}\\
     *&     *&a_{13}&a_{24}&a_{35}&a_{46}&a_{57}&a_{68}&a_{79}\\
     *&a_{12}&a_{23}&a_{34}&a_{45}&a_{56}&a_{67}&a_{78}&a_{89}\\
a_{11}&a_{22}&a_{33}&a_{44}&a_{55}&a_{66}&a_{77}&a_{88}&a_{99}           
\end{bmatrix}.
\]

Computing the band Cholesky factor~$\boldsymbol{R}$ using 
Algorithm~\ref{alg: band Chol}, the number of multiplications is
\begin{multline*}
\sum_{j=1}^n\biggl([j-\max(1,j-\beta)]
+\sum_{i=\max(1,j-\beta)}^j[i-\max(1,j-\beta)]\biggr)\\
	=\sum_{j=1}^n\biggl([j-\max(1,j-\beta)]
	+\sum_{i=0}^{j-\max(1,j-\beta)}i\biggr)
	\le\sum_{j=1}^n\biggl(\beta+\sum_{i=1}^\beta i\biggr)
	=n\bigl[\beta+\tfrac12\beta(\beta+1)\bigr].
\end{multline*}
Assuming that the bandwidth~$\beta$ is small compared to~$n$, we can see that 
the multiplication counts is essentially $\tfrac12n\beta^2$, as is the number 
of addition/subtractions.  There are also $n$ square roots and
\[
\sum_{j=1}^n\bigl[j-\max(1,j-\beta)+1]\le\sum_{j=1}^n(\beta+1)=n(\beta+1)
\]
divisions.

\begin{table}
\caption{Approximate operation counts for solving a band symmetric 
positive-definite linear system when $\beta\ll n$.}

\end{table}




\section{Maximum principle}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\by}{\boldsymbol{y}}
We wish to show that the boundary-value problem~\eqref{eq: Poisson bvp} is
well-posed, generalising the 1D results of \cref{thm: Lu=f apriori infty}.
Consider the general, second-order, linear partial differential operator,
\begin{equation}\label{eq: general L Rn}
(\mathcal{L}u)(\bx)
	=-\sum_{j,k=1}^na_{jk}(\bx)(\partial_j\partial_ku)(\bx)
	+\sum_{j=1}^nb_j(\bx)(\partial_ju)(x)+c(\bx)u(\bx)
\quad\text{for $\bx\in\Omega$,}
\end{equation}
where $\partial_j=\partial/\partial x_j$ and where $\Omega$ is a bounded,
open, connected subset of~$\mathbb{R}^n$.  Since 
$\partial_k\partial_ju=\partial_k\partial_ju$ if $u$ is $C^2$, we assume
without loss of generality that the coefficient matrix is symmetric, that is,
\[
a_{jk}(\bx)=a_{kj}(\bx)\quad\text{for $\bx\in\Omega$.}
\]
In addition, we assume that all coefficients of~$\mathcal{L}$ are bounded
and continuous on~$\Omega$.  

\begin{definition}
The partial differential operator~\eqref{eq: general L Rn} is
\emph{uniformly elliptic} in~$\Omega$ if there exist strictly positive 
constants $\lambda_{\min}$~and $\lambda_{\max}$ such that
\[
\lambda_{\min}|\boldsymbol{\xi}|^2\le\sum_{j,k=1}^na_{jk}(\bx)\xi_j\xi_k
	\le\lambda_{\max}|\boldsymbol{\xi}|^2
\quad\text{for $\bx\in\Omega$ and $\boldsymbol{\xi}\in\mathbb{R}^n$.}
\]
\end{definition}

Equivalently, $\mathcal{L}$ is uniformly elliptic if the spectrum of the 
symmetric matrix~$[a_{ij}(\bx)]$ lies in the 
interval~$[\lambda_{\min},\lambda_{\max}]$ 
for all $\bx\in\Omega$. 

The next two results generalise \cref{lem: Lu<0}~and 
\cref{thm: max principle 1d}.  The proof of the lemma uses the notation
\[
B(\bx_0,\delta)=\{\,\bx\in\mathbb{R}^n:|\bx-\bx_0|<\delta\,\}
\]
for the open ball with centre~$\bx_0$ and radius~$\delta>0$.

\begin{lemma}\label{lem: Lu<0 Omega}
Assume that $\mathcal{L}$ is uniformly elliptic and that $u$ is $C^2$ 
on~$\Omega$.  If $c\ge0$ and $\mathcal{L}u<0$ on~$\Omega$, then $u$ cannot 
attain a non-negative local maximum in~$\Omega$.
\end{lemma}
\begin{proof}
Suppose for a contradiction that there exist $\bx_0\in\Omega$ and $\delta>0$
such that
\[
u(\bx_0)\ge0\quad\text{and}\quad 
\text{$u(\bx)\le u(\bx_0)$ for $\bx\in B(\bx_0,\delta)\subseteq\Omega$.}
\]
Since $u$ has an interior local maximum at~$\bx_0$, it follows that 
$\partial_ju(\bx_0)=0$ and so
\begin{equation}\label{eq: Lu x0}
(\mathcal{L}u)(\bx_0)
	=-\sum_{j,k=1}^na_{jk}(\bx_0)(\partial_j\partial_ku)(\bx_0)
	+c(\bx_0)u(\bx_0).
\end{equation}
Moreover, there is an orthogonal matrix~$\boldsymbol{Q}$ and a diagonal 
matrix~$\boldsymbol{\Lambda}=[\lambda_j\delta_{jk}]$ such that 
\[
\text{$\lambda_{\min}\le\lambda_j\le\lambda_{\max}$ for $1\le j\le n$}
\qquad\text{and}\qquad
\boldsymbol{Q}^T[a_{jk}(\bx_0)]\boldsymbol{Q}=\boldsymbol{\Lambda}.
\]
Let
\[
\bar u(\by)=u(\bx)\quad
\text{for $\by=\bx_0+Q^T(\bx-\bx_0)$ and $\bx\in B(\bx_0,\delta)$,}
\]
noting that $\by\in B(\bx_0,\delta)$ iff $\bx\in B(\bx_0,\delta)$ because
$|\bx-\bx_0|=|\by-\bx_0|$.  Since $\partial y_l/\partial x_j=q_{jl}$, the 
chain rule gives
\[
\partial_ju(\bx)=\sum_{l=1}^n q_{jl}\partial_l\bar u(\by),
\]
and therefore
\begin{align*}
\sum_{j,k=1}^n a_{jk}(\bx_0)(\partial_j\partial_k u)(\bx_0)
	&=\sum_{j,k=1}^n\sum_{l=1}^nq_{jl}\sum_{m=1}^nq_{km}
	\partial_l\partial_m\bar u(\bx_0)\\
	&=\sum_{j,k=1}^n\biggl(\sum_{l,k=1}^n q_{jl}a_{jk}(\bx_0)q_{km}
	\biggr)\partial_l\partial_m\bar u(\bx_0)\\
	&=\sum_{j,k=1}^n\lambda_l\delta_{lm}
	\partial_l\partial_m\bar u(\bx_0)\\
	&=\sum_{j,k=1}^n\lambda_j\delta_{jk}
	\partial_l\partial_m\bar u(\bx_0)
	=\sum_{j=1}^n\lambda_j\partial_j^2\bar u(\bx_0).
\end{align*}
The function~$\bar u$ has a local maximum at~$\bar x_0$ so 
$\partial_j^2\bar u(\bx_0)\le0$ for all~$j$.  But then we see 
from~\eqref{eq: Lu x0} that $(\mathcal{L}u)(\bx_0)<0$, a contradiction.
\end{proof}

\begin{theorem}\label{thm: max principle}
Assume that $\mathcal{L}$ is uniformly elliptic and that $u$ is $C^2$ 
on~$\Omega$.  If $c\ge0$ and $\mathcal{L}u\le0$ on~$\Omega$, then 
\[
\max_{\bx\in\overline{\Omega}}u(\bx)\le\max_{\bx\in\partial\Omega}u^+(\bx).
\]
\end{theorem}
\begin{proof}
Let $\epsilon>0$ and $\mu>0$, and put $w(\bx)=u(\bx)+\epsilon e^{\mu x_1}$.
Since
\begin{align*}
(\mathcal{L}w)(\bx)&=(\mathcal{L}u)(\bx)+\epsilon\bigl[-a_{11}(\bx)\mu^2
	+b_1(\bx)\mu+c(\bx)\bigr]e^{\mu x_1}\\
	&\le-\epsilon\bigl[\lambda_{\min}\mu^2
	-\mu\|b\|_\infty-\|c\|_\infty\bigr]e^{\mu x_1}
\end{align*}
by choosing $\mu$ sufficiently large we can ensure that $\mathcal{L}w<0$
on~$\Omega$.  By \cref{lem:}, the function~$w$ cannot attain a non-negative
local maximum in~$\Omega$, implying that 
\[
u(\bx)\le w(\bx)\le w^+(\bx)\le\max_{by\in\partial\Omega}w^+(\by)
	\le\max_{\by\in\partial\Omega}[u^+(y)+\epsilon e^{\mu L}]
\]
for $\bx\in\Omega$, where $L=\max_{\bx\in\Omega} x_1$.  Since this inequality
holds for any $\epsilon>0$, the result follows.
\end{proof}

\begin{theorem}\label{thm: max min elliptic}
Assume the $\mathcal{L}$ is uniformly elliptic, and that $u$ is continuous 
on~$\overline{\Omega}$ and $C^2$ on~$\Omega$.  If $c\ge0$~and 
$\mathcal{L}u=f$ on~$\Omega$, then
\[
\max_{\overline{\Omega}}u\le\bigl(\max_{\partial\Omega}u^+\bigr)
	+C\max_{\overline{\Omega}}f^+
\]
and
\[
\min_{\overline{\Omega}}u\le\bigl(\min_{\partial\Omega}u^-\bigr)
	+C\min_{\overline{\Omega}}f^-,
\]
where $C=\cosh(\tfrac12\mu\operatorname{diam}\Omega)$~and 
$\mu=\max(1, (1+\|b_1\|_\infty)/\lambda_{\min}$.
\end{theorem}
\begin{proof}
Without loss of generality, we may assume that
$\Omega\subseteq B(\boldsymbol{0},r)$ for $r=\tfrac12\operatorname\Omega$.
Let
\[
w(x)=\bigl(\max_{\partial\Omega}u^+\bigr)+v(x)\max_{\overline{\Omega}}f^+
\quad\text{where}\quad
v(x)=\cos(\mu r)-\cosh(\mu x_1),
\]
and observe that $v\ge0$ on~$\overline{\Omega}$ with
\begin{align*}
(\mathcal{L}v)(x)&=a_{11}(\bx)\mu^2\cosh(\mu x_1)-b_1(\bx)\mu\sinh(\mu x_1)
	+c(\bx)\bigl(v(x)+\max_{\partial\Omega}u^+\bigr)\\
	&\ge\bigl(\lambda_{\min}\mu^2-\|b_1\|_\infty\mu\bigr)\cosh(\mu x_1)
	\ge(\lambda_{\min}\mu-\|b_1\|_\infty)\mu\ge1
\end{align*}
for $\bx\in\Omega$, so
\[
\mathcal{L}(u-w)=f-c(x)\max_{\partial\Omega}u^+
	-(\mathcal{L}v)\max_{\overline{\Omega}}f^+\le0
\quad\text{on $\Omega$.}
\]
By \cref{thm: max principle},
\[
u(\bx)-w(\bx)\le\max_{\partial\Omega}(u-w)^+\quad\text{for $\bx\in\Omega$,}
\]
and we see that
\[
u(\bx)-w(\bx)=\bigl(u(\bx)-\max_{\partial\Omega}u^+\bigr)
	-v(x)\max_{\overline{\Omega}}f^+\le0
	\quad\text{for $\bx\in\partial\Omega$.}
\]
Thus, $u-w\le0$ on~$\Omega$, and therefore
\[
\max_{\overline{\Omega}}u\le\max_{\overline{\Omega}}w
	\le\bigl(\max_{\partial\Omega}u^+\bigr)
	+\cosh(\mu r)\max_{\overline{\Omega}}f^+,
\]
proving the first inequality.  The second follows because 
$\mathcal{L}(-u)=-f$, $(-u)^+=-(u^-)$ and $(-f)^+=-(f^-)$.
\end{proof}

Now consider the elliptic boundary-value problem
\begin{equation}\label{eq: Dirichlet problem L}
\mathcal{L}u=f\quad\text{in $\Omega$,}
	\quad\text{with $u=g$ on $\partial\Omega$.}
\end{equation}
As an immediate consequence of \cref{thm: max min elliptic}, we obtain
the desired \emph{a priori} estimate for~$u$ in terms of the data $f$~and $g$.

\begin{theorem}\label{thm: a priori elliptic}
Assume the $\mathcal{L}$ is uniformly elliptic and that $c\ge0$ on~$\Omega$.
Then any solution $u\in C^2(\Omega)\cap C(\overline{\Omega})$ 
of~\eqref{eq: Dirichlet problem L} satisfies
\[
\max_{\overline{\Omega}}|u|\le\max_{\partial\Omega}|g|
	+C\max_{\overline{\Omega}}|f|.
\]
\end{theorem}

\section{Discrete maximum principle}
We now restrict our attention to a rectangular 
domain~$\Omega=(0,L_x)\times(0,L_y)$ in~$\mathbb{R}^2$, and assume that the
coefficients of $\mathcal{L}$ satisfy
\[
a_{jk}(x,y)=a_j(x,y)\delta_{jk}\quad\text{and}\quad
b_j(x,y)=0\quad\text{for $j$, $k\in\{1,2\}$.}
\]
Thus,
\begin{equation}\label{eq: L simple}
(\mathcal{L}u)(x,y)=-a_1(x,y)(\partial_1^2u)(x,y)
	-a_2(x,y)(\partial_2^2u)(x,y)+c(x,y)u(x,y),
\end{equation}
and we define the corresponding finite difference operator 
\begin{multline*}
(\mathcal{L}_{\Delta x,\Delta y}U)_{pq}=-a_1(x_p,y_q)\,
	\frac{U_{p+1,q}-2U_{pq}+U_{p-1,q}}{\Delta x^2}\\
	-a_2(x_p,y_q)\,
	\frac{U_{p,q+1}-2U_{pq}+U_{p,q-1}}{\Delta y^2}
	+c(x_p,y_q)U_{pq}.
\end{multline*}
The finite difference approximation to the boundary-value 
problem~\eqref{eq: Dirichlet problem L} can then be written as
\begin{equation}\label{eq: discrete bvp 2d}
\begin{aligned}
(\mathcal{L}_{\Delta x,\Delta y}U)_{pq}&=f_{pq}&&
\text{for $(x_p,y_q)\in\Omega$,}\\
U_{pq}&=g_{pq}&&\text{for $(x_p,y_q)\in\Gamma$,}
\end{aligned}
\end{equation}
which, in the special case $a_1(x,y)=1$, $a_2(x,y)=1$, $c(x,y)=0$
reduces to the five-point scheme \eqref{eq: 5-pt Poisson} for the Poisson

The simplified operator~\eqref{eq: L simple} is uniformly elliptic iff
there are positive constants $\lambda_{\min}$ and $\lambda_{\max}$ such that
\[
\lambda_{\min}\le a_1(x,y)\le\lambda_{\max}\quad\text{for $(x,y)\in\Omega$,}
\]
and we will assume throughout that this is the case.  Given a grid 
point~$(x_{p^*},y_{q^*})\in\Omega$, let
\[
\operatorname{Nbr}(p^*,q^*)=\{(p+1,q),(p-1,q),(p,q+1),(p,q-1)\}.
\]
so that $(p,q)\in\operatorname{Nbr}(p^*,q^*)$ if and only if $(x_p,y_q)$ is 
one of the four nearest neighbours to~$(x_{p^*},y_{q^*})$.  The following 
discrete analogue of \cref{lem: Lu<0 Omega}.

\begin{lemma}\label{lem: Lu<0 Omega discrete}
If $c_{p,q}\ge0$ and $(\mathcal{L}_{\Delta x,\Delta y}U)_{p,q}<0$ for
$(x_p,y_q)\in\Omega$, then there is no grid point~$(x_{p^*},y_{q^*})\in\Omega$
such that
\[
U_{p^*,q^*}\ge0\quad\text{and}\quad\text{$U_{p,q}\le U_{p^*,q^*}$ 
for $(p,q)\in\operatorname{Nbr}(p^*,q^*)$.}
\]
\end{lemma}
\begin{proof}
If such a grid point exists, then
\[
U_{p^*+1,q^*}+U_{p^*-1,q^*}\le 2U_{p^*,q^*}
\quad\text{and}\quad
U_{p^*,q^*+1}+U_{p^*,q^*-1}\le 2U_{p^*,q^*},
\]
so $(\mathcal{L}_{\Delta x,\Delta y}U)_{p^*,q^*}\ge0$, contradicting the
second hypothesis of the lemma.
\end{proof}

A discrete analogue of \cref{thm: max principle} then follows; we use the
abbreviation
\[
\max_{S}U^+=\max_{(x_p,y_q)\in S}U_{p,q}
	\quad\text{for $S\subseteq\overline{\Omega}.$}
\]

\begin{theorem}
If $c_{p,q}\ge0$ and $(\mathcal{L}_{\Delta x,\Delta y}U)_{p,q}\le0$
for $(x_p,y_q)\in\Omega$, then
\[
\max_\Omega U\le\max_{\partial\Omega}U^+ \quad\text{for $(x_p,y_q)\in\Omega$.}
\]
\end{theorem}
\begin{proof}
Let $\epsilon>0$ and $\mu>0$, and put 
\[
W_{p,q}=U_{p,q}+\epsilon g(x_p)\quad\text{where $g(x)=e^{\mu x}$.}
\]
Since $g^{(4)}(x)=\mu^4e^{\mu x}\ge0$ for all~$x$, if follows by considering
the remainder term for the Taylor expansion of~$g$ about~$x_p$ that
\[
\frac{g(x_{p+1}-2g(x_p)+g(x_{p-1}}{\Delta x^2}\ge g''(x_p)=\mu^2 e^{\mu x_p}.
\]
Hence, writing $a_{1,p,q}=a_1(x_p,y_q)$ and $c_{p,q}=c(x_p,y_q)$,
\begin{align*}
(\mathcal{L}_{\Delta x,\Delta y}W)_{p,q}
	&=(\mathcal{L}_{\Delta x,\Delta y}U)_{p,q}+\epsilon\biggl(-a_{1,p,q}
\frac{g(x_{p+1}-2g(x_p)+g(x_{p-1}}{\Delta x^2}+c_{p,q}g(x_p)\biggr)\\
	&\le(\mathcal{L}_{\Delta x,\Delta y}U)_{p,q}+\epsilon\bigl(-a_{1,p,q}
e^{\mu x_p}+c_{p,q}e^{\mu x_p}\bigr)\\
	&\le0-\epsilon\bigl(\lambda_{\min}\mu^2-\|c\|_\infty\bigr)e^{\mu x_p},
\end{align*}
and by choosing $\mu^2>\|c\|_\infty/\lambda_{\min}$ we can ensure that
$(\mathcal{L}_{\Delta x,\Delta y}W)_{p,q}<0$ for $(x_p,y_q)\in\Omega$.
Applying \cref{lem: Lu<0 Omega discrete}, we conclude that
\[
\max_\Omega U\le\max_\Omega W\le\max_{\partial\Omega}W
	\le\bigl(\max_{\partial\Omega}U\bigr)+\epsilon e^{\mu L_x}.
\]
Since this inequality holds for any $\epsilon>0$, the result follows.
\end{proof}

Next is a discrete version of \cref{thm: max min elliptic}.

\begin{theorem}\label{thm: discrete max principle elliptic}
If $c_{p,q}\ge0$ and $(\mathcal{L}_{\Delta x,\Delta y}U)_{p,q}=f_{p,q}$
for $(x_p,y_q)\in\Omega$, then
\[
\max_\Omega U\le\max_{\partial\Omega}U^++C\max_{(x_p,y_q)\in\Omega} f^+_{p,q}.
\]
and
\[
\min_\Omega U\le\min_{\partial\Omega}U^-+C\max_{(x_p,y_q)\in\Omega}f^-_{p,q},
\]
where $C=L^2/(8\lambda_{\min})$.
\end{theorem}
\begin{proof}
Let
\[
W_{p,q}=\bigl(\max_{\partial\Omega}U^+\bigr)
	+v(x_p)\max_\Omega f^+_{\cdot,\cdot}
\quad\text{where $v(x)=\frac{x(L_x-x)}{2\lambda_{\min}}$.}
\]
Since the second-order central difference quotient equals the exact second
derivative for a quadratic polynomial,
\[
\frac{v(x_{p+1})-2v(x_p)+v(x_{p-1})}{\Delta x^2}=v''(x_p)
	=\frac{-1}{\lambda_{\min}}
\]
and thus, noting that $v(x)\ge0$ for~$0\le x\le L_x$, we conclude that
\[
(\mathcal{L}_{\Delta x,\Delta y}v)_{p,q}=\frac{a_{1,p,q}}{\lambda_{\min}}
	+c_{p,q}v_{p,q}\ge1\quad\text{for $(x_p,y_q)\in\Omega$.}
\]
It follows that
\[
(\mathcal{L}_{\Delta x,\Delta y}W)_{p,q}
	=c_{p,q}\bigl(\max_{\partial\Omega}U^+\bigr)
	+(\mathcal{L}_{\Delta x,\Delta y}V)_{p,q}
	\bigl(\max_\Omega f^+_{\cdot,\cdot}\bigr)
	\ge\bigl(\max_\Omega f^+_{\cdot,\cdot}\bigr)
\]
and so
\[
\bigl(\mathcal{L}_{\Delta x,\Delta y}(U-W)\bigr)_{p,q}
	=f_{p,q}-(\mathcal{L}_{\Delta x,\Delta y}W)_{p,q}
	\le f_{p,q}-\max_{\Omega}f^+_{\cdot,\cdot}\le0
\quad\text{for $(x_p,y_q)\in\Omega$,}  
\]
with
\[
(U-W)_{p,q}=U_{p,q}-\bigl(\max_{\partial\Omega}U^+\bigr)
	-V_p\max f^+_{\cdot,\cdot}\le0
\quad\text{for $(x_p,y_q)\in\partial\Omega$.}  
\]
By \cref{thm: discrete max principle elliptic},
\[
\max_\Omega(U-W)\le\max_{\partial\Omega}(U-W)\le0,
\]
and the first inequality follows because $\max_{0\le x\le L_x}v(x)=C$ and so
\[
\max_\Omega U\le\max_\Omega W\le\bigl(\max_{\partial\Omega}U^+\bigr)
	+C\max_\Omega f^+_{\cdot,\cdot}.
\]
The second follows because
$\bigl(\mathcal{L}_{\Delta x,\Delta y}(-U)\bigr)_{p,q}=-f_p$,
$(-U)^+_{p,q}=-(U^+_{p,q})$ and $(-f)^+_{p,q}=-(f^+_{p,q})$.
\end{proof}

Our final result for this section is a discrete version of 
\cref{thm: a priori elliptic}.

\begin{theorem}\label{thm: discrete apriori elliptic}
If $c\ge0$ on $\Omega$, then the discrete boundary-value 
problem~\eqref{eq: discrete bvp 2d} has a unique solutions~$U_{p,q}$, and
\[
\max_{\overline{\Omega}}|U|\le\max_{\partial\Omega}|g_{\cdot,\cdot}|
	+C\max_\Omega|f_{\cdot,\cdot}|.
\]
\end{theorem}
\begin{proof}
The \emph{a priori} estimate follows at once from 
\cref{thm: discrete max principle elliptic}, and shows that if $f_{p,q}=0$
for all $(x_p,y_q)\in\Omega$~and if $g_{p,q}=0$ for all 
$(x_p,y_q)\in\partial\Omega$, then the problem has only the trivial solution
$U_{p,q}=0$ for all $(x_p,y_q)\in\overline{\Omega}$. Hence, the coefficient
matrix for the associated linear system is non-singular, implying the
existence and uniqueness of~$U_{p,q}$.
\end{proof}

\section{An error bound}

We will say that the finite difference scheme used in~\eqref{eq: }
is \emph{stable} if there is a constant~$C$ --- independent of $f$, $g$,
$\Delta x$~and $\Delta y$ --- such that
\begin{equation}\label{eq: stability elliptic Omega}
\max_{\overline{\Omega}}|U|\le C\Bigl(\max_{\partial\Omega}|g_{\cdot,\cdot}|.
\end{equation}
For example, by \cref{thm: discrete apriori elliptic}, sufficient conditions
for stability are that the simplified partial differential 
operator~\eqref{eq: L simple} is uniformly elliptic with~$c\ge0$.  

We define the local truncation error in the usual way, as
\[
\tau_{p,q}=f_{p,q}-(\mathcal{L}_{\Delta x,\Delta y}u)_{p,q}
	=(\mathcal{L}u)_{p,q}-(\mathcal{L}_{\Delta x,\Delta y}u)_{p,q},
\]
and note that
\begin{equation}\label{eq: tau pq bound}
\begin{aligned}
\tau_{p,q}&=-a_{1,p,q}\biggl(u_{xx}(x_p,y_q)
	-\frac{u(x_{p+1},y_q)-2u(x_p,y_q)+u(x_{p-1},y_q)}{\Delta x^2}\biggr)\\
	&\qquad{}-a_{2,p,q}\biggl(u_{yy}(x_p,y_q)
	-\frac{u(x_p,y_{q+1})-2u(x_p,y_q)+u(x,y_{q-1})}{\Delta y^2}\biggr),
\end{aligned}
\end{equation}
so, by \cref{thm: 2nd central diff},
\begin{equation}\label{eq: tau pq bound}
|\tau_{p,q}|\le\biggl(
\frac{|a_{1,p,q}|}{12}\|\partial_1^4u\|_\infty\,\Delta x^2
+\frac{|a_{2,p,q}|}{12}\|\partial_2^4u\|_\infty\,\Delta y^2\biggr)
\end{equation}
The \emph{solution error},
\[
E_{p,q}=U_{p,q}-u(x_p,y_q),
\]
satisfies
\[
(\mathcal{L}_{\Delta x,\Delta y}E)_{p,q}
	=(\mathcal{L}_{\Delta x,\Delta y}U)_{p,q}
	-(\mathcal{L}_{\Delta x,\Delta y}u)_{p,q}
	=f_{p,q}-(\mathcal{L}_{\Delta x,\Delta y}u)_{p,q}=\tau_{p,q},
\]
for $(x_p,y_q)\in\Omega$, with $E_{p,q}=g_{p,q}-g_{p,q}=0$
for $(x_p,y_q)\in\partial\Omega$.  Replacing $U$, $f$ and $g$ 
in~\eqref{eq: stability elliptic Omega} by $E$, $\tau$ and $0$, respectively,
we have
\[
\max_{\overline{\Omega}}|U|\le C\max_\Omega|\tau_{\cdot,\cdot}|.
\]
Combining this estimate with~\eqref{eq: tau pq bound}, we obtain the error
bound
\[
|U_{p,q}-u(x_p,y_q)|\le C\biggl(
\frac{\|a_1\|_\infty}{12}\|\partial_1^4u\|_\infty\,\Delta x^2
+\frac{\|a_2\|_\infty}{12}\|\partial_2^4u\|_\infty\,\Delta y^2\biggr)
\]
for $(x_p,y_q)\in\overline{\Omega}$.  In other words,
\[
U_{p,q}=u(x_p,y_q)+O(\Delta x^2+\Delta y^2),
\]
showing that the finite difference scheme is indeed second-order 
accurate.

\section{Parabolic problems in 2D}
Consider the following 2D initial-boundary value problem,
\begin{equation}\label{eq: ibvp 2d}
\begin{aligned}
u_t-a\nabla^2u&=f(x,y,t)&&\text{for $(x,y)\in\Omega$ and $0<t<T$,}\\
u&=g(x,y,t)&&\text{for $(x,y)\in\partial\Omega$ and $0<t<T$,}\\
u&=u_0(x,y)&&\text{for $(x,y)\in\Omega$ when $t=0$,}
\end{aligned}
\end{equation}
where, for simplicity, we assume that the coefficient~$a$ is a positive
constant.  For the rectangular domain~\eqref{eq: Omega rectangle} and the
grid \eqref{eq: xp yq grid}, we can define the semidiscrete finite difference
solution~$U_{pq}(t)\approx u(x_p,y_q,t)$ by
\[
\frac{dU_{p,q}}{dt}-a\biggl(\frac{U_{p+1,q}-2U_{p,q}+U_{p-1,q}}{\Delta x^2}
	+\frac{U_{p,q+1}-2U_{p,q}+U_{p,q-1}}{\Delta y^2}\biggr)=f_{p,q}(t)
\]
for $1\le p\le P-1$ and $1\le q\le Q-1$, together with the discrete
boundary conditions
\[
U_{p,q}(t)=g(x_p,y_q,t)\quad
	\text{for $(x_p,y_q)\in\partial\Omega$ and $1\le t\le T$,}
\]
and initial condition
\[
U_{p,q}(0)=u_0(x_p,y_q)\quad\text{for $1\le p\le P-1$ and $1\le q\le Q-1$.}
\]
Since
\[
-a\bigl(\delta_x^2U_{p,q}+\delta_y^2U_{p,q}\bigr)=f_{p,q}-\frac{dU_{p,q}}{dt},
\]
we see that
\[
\boldsymbol{A}\boldsymbol{U}=\boldsymbol{f}(t)+\boldsymbol{g(t)}
	-\frac{d\boldsymbol{U}}{dt}
\]
where $\boldsymbol{A}$ is the same matrix as in \cref{thm: Poisson matrix},
and the vectors $\boldsymbol{f}(t)$~and $\boldsymbol{g}(t)$ are time-dependent
vectors constructed from grid values of the functions $f$~and $g$, as in 
\cref{example: 5-pt matrix}.  Thus, in matrix form the semidiscrete method is
\[
\frac{d\boldsymbol{U}}{dt}+\boldsymbol{A}\boldsymbol{U}
	=\boldsymbol{f}(t)+\boldsymbol{g(t)} 
\quad\text{for $0\le t\le T$, with $\boldsymbol{U}(0)=\boldsymbol{U}_0$.}
\]
Here, the column vector~$\boldsymbol{U}_0=[(U_0)_j]_{j=1}^M$ is defined
by putting $(U_0)_j=u_0(x_p,y_q)$ for~$j$ as in~\eqref{eq: column-major}.

By introducing the time levels~\eqref{eq: uniform tn}, we can consider a
fully-discrete approximation
\[
U^n_{p,q}\approx U_{p,q}(t_n)\approx u(x_p,y_q,t_n).
\]
The forward Euler method for the 2D problem~\eqref{eq: ibvp 2d} is
\[
\frac{U^{n+1}_{p,q}-U^n_{p,q}}{\Delta t}
	-a\biggl(\frac{U_{p+1,q}^n-2U_{p,q}^n+U_{p-1,q}^n}{\Delta x^2}
	+\frac{U_{p,q+1}^n-2U_{p,q}^n+U_{p,q-1}^n}{\Delta y^2}\biggr)=f_{p,q}^n,
\]
for $0\le n\le N-1$, $1\le p\le P-1$ and $1\le q\le Q-1$, with the 
fully-discrete boundary and initial conditions
\begin{equation}\label{eq: fully discrete bc ic 2d}
\begin{aligned}
U^n_{p,q}&=g^n_{p,q}&
&\text{for $0\le n\le N-1$ and $(x_p,y_q)\in\partial\Omega$,}\\
U^0_{p,q}&=(U_0)_{p,q}&
&\text{for $(x_p,y_q)\in\Omega$.}
\end{aligned}
\end{equation}
Let
\[
\rho_x=\frac{a\,\Delta t}{\Delta x^2}\quad\text{and}\quad
\rho_y=\frac{a\,\Delta t}{\Delta y^2},
\]
so that
\[
U^{n+1}_{p,q}-U^n_{p,q}-\rho_x\bigl(U^n_{p+1,q}-2U^n_{p,q}+U^n_{p-1,q}\bigr)
	-\rho_y\bigl(U^n_{p,q+1}-2U^n_{p,q}+U^n_{p,q-1}\bigr)
	=f^n_{p,q}
\]
and hence
\[
U^{n+1}_{p,q}=f^n_{p,q}+\rho_xU^n_{p-1,q}+\rho_yU^n_{p,q-1}
	+(1-2\rho_x-2\rho_y)U^n_{p,q}+\rho_yU^n_{p,q+1}+\rho_xU^n_{p+1,q}.
\]
The proof of \cref{thm: explicit Euler stability} generalises to
show that the scheme is stable if $1-2\rho_x-2\rho_y\ge0$, or equivalently if
\[
\Delta t\le\frac{\Delta x^2\,\Delta y^2}{2a(\Delta x^2+\Delta y^2)}
	=\frac{\Delta x\,\Delta y}{2a(\theta+\theta^{-1})},
\quad\text{where $\theta=\frac{\Delta x}{\Delta y}$.}
\]

The backward Euler method for the 2D problem~\eqref{eq: ibvp 2d} is
\[
\frac{U^n_{p,q}-U^{n-1}_{p,q}}{\Delta t}
	-a\biggl(\frac{U_{p+1,q}^n-2U_{p,q}^n+U_{p-1,q}^n}{\Delta x^2}
	+\frac{U_{p,q+1}^n-2U_{p,q}^n+U_{p,q-1}^n}{\Delta y^2}\biggr)=f_{p,q}^n,
\]
for $1\le n\le N$, $1\le p\le P-1$ and $1\le q\le P-1$, with the boundary 
and initial conditions again given by~\eqref{eq: fully discrete bc ic 2d}.
Rearranging the finite difference equation gives
\[
-\rho_yU^n_{p,q-1}-\rho_xU^n_{p-1,q}+(1+2\rho_x+2\rho_y)U^n_{p,q}
-\rho_xU^n_{p+1,q}-\rho_yU^n_{p,q+1}=U^{n-1}_{p,q}+f^n_{p,q}\,\Delta t.
\]
In matrix notation, the backward Euler method looks the same as in 1D,
\[
\frac{\boldsymbol{U}^n-\boldsymbol{U}^{n-1}}{\Delta t}
	+\boldsymbol{A}\boldsymbol{U}^n=\boldsymbol{f}^n+\boldsymbol{g}^n,
\]
and at the $n$th time step we must again solve the linear system
\[
(\boldsymbol{I}+\Delta t\,\boldsymbol{A})\boldsymbol{U}^n
	=\boldsymbol{U}^{n-1}+\Delta t(\boldsymbol{f}^n+\boldsymbol{g}^n).
\]
However, $\boldsymbol{A}$ is now given by \cref{thm: Poisson matrix} and the 
meanings of the vectors $\boldsymbol{U}^n$, 
$\boldsymbol{f}^n=\boldsymbol{f}(t_n)$ and 
$\boldsymbol{g}^n=\boldsymbol{g}(t_n)$ are also different from in
\eqref{eq: implicit Euler 1d vector}.  The coefficient matrix is again 
symmetric and positive-definite, with the same sparsity pattern 
as~$\boldsymbol{A}$, so we can use a band Cholesky factorization,
\begin{equation}\label{eq: backward Euler 2d Cholesky}
\boldsymbol{I}+\Delta t\,\boldsymbol{A}=\boldsymbol{R}^T\boldsymbol{R},
\end{equation}
to compute $\boldsymbol{U}^n$ at the $n$th~time step.

\begin{Exercises}
\exercise
Suppose that the matrices $\boldsymbol{A}$~and $\boldsymbol{B}$ have upper 
bandwidth $s_{\boldsymbol{A}}$~and $s_{\boldsymbol{B}}$, respectively, and that 
the number of columns of~$\boldsymbol{A}$ equals the number of rows 
of~$\boldsymbol{B}$.  Show that the matrix 
product~$\boldsymbol{A}\boldsymbol{B}$ has upper 
bandwidth~$\max(s_{\boldsymbol{A}},s_{\boldsymbol{B}})$.  State and prove the 
corresponding result for lower bandwidths.

\exercise
Write down $\boldsymbol{A}_{\mathrm{band}}$ given 
by~\eqref{eq: symm band storage} if
\[
\boldsymbol{A}=\begin{bmatrix}
 5& 1& 7&  &  &\\
 1& 2& 0&-1&  &\\
 7& 0& 8& 2& 3&\\
  &-1& 2& 5& 1& 6\\
  &  & 3& 1& 7& 2\\
  &  &  & 6& 2& 9
\end{bmatrix}.
\]
\begin{ans}
\[
\boldsymbol{A}_{\mathrm{band}}=\begin{bmatrix}
 *& *& 7&-1& 3& 6\\
 *& 1& 0& 2& 1& 2\\
 5& 2& 8& 5& 7& 9                                
\end{bmatrix}
\]
\end{ans}

\exercise
Write an in-place version of Algorithm~\ref{alg: band Chol}.
\begin{ans}
In-place algorithm:
\begin{center}
\newcommand{\rb}[1]{r^{\mathrm{band}}_{#1}}
\begin{algorithmic}
\Require{The $(\beta+1)\times n$ 
array~$\boldsymbol{R}_{\mathrm{band}}=[\rb{ij}]$
stores a real, $n\times n$, symmetric positive-definite matrix with 
bandwidth~$\beta$, using the symmetric band storage so
$a_{ij}=\rb{\beta+1+i-j,j}$.}
\State
\Function{Factorize}{$\boldsymbol{R}_{\mathrm{band}}$}
\For{$j=1:n$}
    \For{$i=\max(1,j-\beta):j$}
        \State $s=0$
        \For{$k=\max(1,j-\beta):i-1$}
            \State $s=s+\rb{\beta+1+k-i,i}\rb{\beta+1+k-j,j}$
        \EndFor
        \State $\rb{\beta+1+i-j,j}=(\rb{\beta+1+i-j,j}-s)/\rb{\beta+1,i}$
    \EndFor
    \State $s=0$
    \For{$k=\max(1,j-\beta):j-1$}
        \State $s=s+(\rb{\beta+1+k-j,j})^2$
    \EndFor
    \If{$\rb{s+1,j}\le s$} 
        \State Error: $\boldsymbol{A}$ is not positive-definite.
    \EndIf
    \State $\rb{\beta+1,j}=\sqrt{\rb{\beta+1,j}-s}$
\EndFor
\State\Return{$\boldsymbol{R}_{\mathrm{band}}$}
\EndFunction
\end{algorithmic}
\end{center}
\end{ans}


\exercise
Write an in-place version of Algorithm~\ref{alg: band Chol solve}.
\begin{ans}
In-place algorithm:
\begin{center}
\newcommand{\rb}[1]{r^{\mathrm{band}}_{#1}}
\begin{algorithmic}
\Require{$\boldsymbol{x}=[b_i]_{i=1}^n$ stores the right-hand side vector.}
\Require{The $(\beta+1)\times n$ array 
$\boldsymbol{R}_{\mathrm{band}}=[\rb{ij}]$ stores 
the band Cholesky factor~$\boldsymbol{R}=[r_{ij}]$ of~$\boldsymbol{A}$, as 
computed via Algorithm~\ref{alg: band Chol} so that 
$r_{ij}=\rb{\beta+1+i-j,j}$.}
\State
\Function{Solve}{$\boldsymbol{x},\boldsymbol{R}_{\mathrm{band}}$}
\For{$i=1:n$}
    \State $s=x_i$
    \For{$j=\max(1,i-\beta):i-1$}
        \State $s=s-\rb{\beta+1+j-i,i}x_j$
    \EndFor
    \State $x_i=s/\rb{\beta+1,i}$
\EndFor
\For{$i=n:-1:1$}
    \State $s=x_i$
    \For{$j=i+1:\min(n,i+\beta)$}
        \State $s=s-\rb{\beta+1+i-j,j}x_j$
    \EndFor
    \State $x_i=s/\rb{\beta+1,i}$
\EndFor
\State\Return{$\boldsymbol{x}$}
\EndFunction
\end{algorithmic}
\end{center}
\end{ans}

\exercise
Describe the 2D version of the Crank--Nicolson method, and count the number of 
floating-point operations needed for each of the following computations.
\begin{description}
\item{(i)} Finding the Cholesky factor~$\boldsymbol{R}$ 
in~\eqref{eq: backward Euler 2d Cholesky}.
\item{(ii)} Solving for~$\boldsymbol{U}^n$ for a single choice of~$n$.
\item{(iii)} Completing the whole calculation.
\item{(iv)} How does the computational cost scale with the overall problem 
size~$S=NPQ$?
\end{description}

\end{Exercises}
