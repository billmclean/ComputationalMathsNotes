\chapter{Finite differences in 2D}\label{chap: finite diff 2d}

In this chapter, we study the numerical solution of the Dirichlet boundary-value 
problem for the Poisson equation. Let $\Omega$ be a bounded, open subset 
of~$\mathbb{R}^2$, with a piecewise smooth boundary~$\Gamma=\partial\Omega$.  
Given suitable functions $f(x,y)$~and $g(x,y)$, we see $u=u(x,y)$ satisfying
\begin{equation}\label{eq: Poisson bvp}
\begin{aligned}
-\nabla^2u&=f(x,y)&&\text{for $(x,y)\in\Omega$,}\\
u&=g(x,y)&&\text{for $(x,y)\in\Gamma$.}
\end{aligned}
\end{equation}
Here, the \emph{Laplacian} is the second-order elliptic differential operator
defined by
\[
\nabla^2 u=\nabla\cdot(\nabla u)=\frac{\partial^2u}{\partial x^2}
    +\frac{\partial^2u}{\partial y^2}.
\]

\section{Maximum principle}

\section{Five-point difference scheme}
For simplicity, we now restrict our attention to the case when the spatial 
domain is a rectangle,
\[
\Omega=(0,L_x)\times(0,L_y).
\]
To set up the spatial finite difference grid, we fix positive integers $P$~and 
$Q$, define the step sizes
\[
\Delta x=\frac{L_x}{P}\quad\text{and}\quad\Delta y=\frac{L_y}{Q},
\]
and define the grid points
\[
(x_p,y_q)=(p\,\Delta x,q\,\Delta y)
\quad\text{for $0\le p\le P$ and $0\le q\le Q$.}
\]
Our is to compute $U_{p,q}\approx u(x_p,y_q)$ where $u$ is the solution 
of~\eqref{eq: Poisson bvp}.

Let $\delta_x^2$~and $\delta_y^2$ denote the second-order, central difference 
operators in the $x$- and $y$-directions, respectively; that is,
\[
\delta_x^2u(x,y)=\frac{u(x+\Delta x,y)-2u(x,y)+u(x-\Delta x,y)}{\Delta x^2}
    =\frac{\partial^2u}{\partial x^2}+O(\Delta x^2)
\]
and
\[
\delta_y^2u(x,y)=\frac{u(x,y+\Delta y)-2u(x,y)+u(x,y-\Delta y)}{\Delta x^2}
    =\frac{\partial^2u}{\partial y^2}+O(\Delta y^2).
\]
We also write
\[
\delta_x^2U_{pq}=\frac{U_{p+1,q}-2U_{p,q}+U_{p-1,q}}{\Delta x^2}
\quad\text{and}\quad
\delta_y^2U_{pq}=\frac{U_{p,q}-2U_{p,q}+U_{p,q-1}}{\Delta y^2}.
\]
With this notation, our finite difference scheme can be written compactly as
\begin{equation}\label{eq: 5-pt Poisson}
\begin{aligned}
-\bigl(\delta_x^2U_{p,q}+\delta_y^2U_{p,q}\bigr)&=f_{p,q}
    &&\text{for $(x_p,y_q)\in\Omega$,}\\
U_{p,q}&=g_{p,q}&&\text{for $(x_p,y_q)\in\Gamma$,}
\end{aligned}
\end{equation}
with the obvious abbreviations $f_{p,q}=f(x_p,y_q)$~and $g_{p,q}=g(x_p,y_q)$.
Notice that $(x_p,y_q)\in\Omega$ for $1\le p\le P-1$ and $1\le q\le Q-1$, so 
there are 
\[
M=(P-1)(Q-1)
\]
unknown values of~$U_{p,q}$ at the interior grid points.  The remaining
$(P+1)(Q+1)-M=2P+2Q$ values are given directly by the Dirichlet boundary 
condition.  The finite difference approximation provides one equation for 
each interior grid points, and hence one equation for each unknown, to yield an 
$M\times M$~linear system.  Figure~\ref{fig: 5-pt stencil} shows the stencil 
for the scheme, which involes 5~grid points: $(x_p,y_q)$ and its four nearest 
neighbours $(x_{p-1},y_q)$, $(x_{p+1},y_q)$, $(x_{p,q-1})$~and 
$(x_p,y_{q+1})$.

\begin{figure}
\caption{Five-point finite difference stencil for the discrete Poisson 
equation~\eqref{eq: 5-pt Poisson}.}\label{fig: 5-pt stencil}
\begin{center}
\begin{tikzpicture}[scale=0.5]
\draw[->] (-1,0) -- (17,0);
\node[right] at (17,0) {$x$};
\node[below] at (8,0) {$x_p$};
\draw[->] (0,-1) -- (0,13);
\node[above left] at (0,13) {$y$};
\node[left] at (0,6) {$y_q$};
\foreach \x in {2, 4, ..., 14}
    \draw[thin] (\x,0) -- (\x,12);
\foreach \y in {2, 4, ..., 10}
    \draw[thin] (0,\y) -- (16,\y);
\draw[ultra thick] (8,4) -- (8,8);
\draw[ultra thick] (6,6) -- (10,6);
\draw[fill=red] (8,4)  circle (0.15cm);
\draw[fill=red] (8,6)  circle (0.15cm);
\draw[fill=red] (8,8)  circle (0.15cm);
\draw[fill=red] (6,6) circle (0.15cm);
\draw[fill=red] (10,6) circle (0.15cm);
\node[below] at (16,0) {$L_x$};
\node[left]  at (0,12) {$L_y$};
\draw[thick] (0,0) -- (16,0) -- (16,12) -- (0,12) -- (0,0);
\end{tikzpicture}
\end{center}
\end{figure}

To describe the $M\times M$ system explicitly, we need to arrange the 
unknowns~$U_{p,q}$ into a column vector of length~$M$.  A standard approach is 
to think of the $U_{p,q}$ as the entries of a $(P-1)\times(Q-1)$ matrix and use 
\emph{column-major ordering}, so that
\begin{equation}\label{eq: column-major}
U_j=U_{p,q}\quad
\text{where $j=p+(q-1)(P-1)$ for $1\le p\le P-1$ and $1\le q\le Q-1$.}
\end{equation}
The right-hand sides $f_{p,q}$ are arranged in the same way.  

\begin{example}\label{example: 5-pt matrix}
Suppose $P=5$ and $Q=4$, with $\Delta x=h=\Delta y$.  The finite difference 
equation is then
\[
-\frac{1}{h^2}\bigl(U_{p+1,q}-2U_{p,q}+U_{p-1,q}+U_{p,q+1}2-U_{p,q}+U_{p,q-1}
    \bigr)=f_{p,q},
\]
or equivalently,
\[
\frac{1}{h^2}\bigl(-U_{p,q-1}-U_{p-1,q}+4U_{p,q}-U_{p+1,q}-U_{p,q+1}\bigr)
    =f_{p,q}.
\]
When $p=1$~and $q=3$,
\[
\frac{1}{h^2}\bigl(-U_{1,2}-U_{0,3}+4U_{1,3}-U_{2,3}-U_{1,4}\bigr)
    =f_{1,3},
\]
and the boundary conditions give $U_{0,3}=g_{0,3}$~and $U_{1,4}=g_{1,4}$, so
\[
\frac{1}{h^2}\bigl(-U_{1,2}+4U_{1,3}-U_{2,3}\bigr)
    =f_{1,3}+\frac{1}{h^2}\bigl(g_{0,3}+g_{1,4}\bigr).
\]
Using column-major ordering~\eqref{eq: column-major} with~$P=5$, we have
$U_5=U_{1,2}$, $U_9=U_{1,3}$~and $U_{10}=U_{2,3}$ so the $9$th equation is
\[
\frac{1}{h^2}\bigl(-U_5+4U_9-U_{10}\bigr)=f_9
    +\frac{1}{h^2}\bigl(g_{0,3}+g_{1,4}\bigr).
\]
Figure~\ref{fig: 5-pt matrix} shows the complete $12\times12$ linear system.
\end{example}

\begin{figure}
\caption{The $12\times12$ linear system from 
Example~\ref{example: 5-pt matrix}.}\label{fig: 5-pt matrix}
\begin{gather*}
\frac{1}{h^2}
\left[\begin{array}{cccc|cccc|cccc}
 4&-1& 0& 0&  -1& 0& 0& 0&   0& 0& 0& 0\\
-1& 4&-1& 0&   0&-1& 0& 0&   0& 0& 0& 0\\
 0&-1& 4&-1&   0& 0&-1& 0&   0& 0& 0& 0\\
 0& 0&-1& 4&   0& 0& 0&-1&   0& 0& 0& 0\\
\hline
-1& 0& 0& 0&   4&-1& 0& 0&  -1& 0& 0& 0\\
 0&-1& 0& 0&  -1& 4&-1& 0&   0&-1& 0& 0\\
 0& 0&-1& 0&   0&-1& 4&-1&   0& 0&-1& 0\\
 0& 0& 0&-1&   0& 0&-1& 4&   0& 0& 0&-1\\
\hline
 0& 0& 0& 0&  -1& 0& 0& 0&   4&-1& 0& 0\\
 0& 0& 0& 0&   0&-1& 0& 0&  -1& 4&-1& 0\\
 0& 0& 0& 0&   0& 0&-1& 0&   0&-1& 4&-1\\
 0& 0& 0& 0&   0& 0& 0&-1&   0& 0&-1& 4
\end{array}\right]
\left[\begin{array}{c}
U_{1,1}\\ U_{2,1}\\ U_{3,1}\\ U_{4,1}\\ 
\hline
U_{1,2}\\ U_{2,2}\\ U_{3,2}\\ U_{4,2}\\                
\hline
U_{1,3}\\ U_{2,3}\\ U_{3,3}\\ U_{4,3}                
\end{array}\right]\\
=\left[\begin{array}{c}
f_{1,1}\\ f_{2,1}\\ f_{3,1}\\ f_{4,1}\\                
\hline
f_{1,2}\\ f_{2,2}\\ f_{3,2}\\ f_{4,2}\\                
\hline
f_{1,3}\\ f_{2,3}\\ f_{3,3}\\ f_{4,3}                
\end{array}\right]
+\frac{1}{h^2}\left[\begin{array}{c}
g_{01}+g_{10}\\ g_{20}\\ g_{30}\\ g_{40}+g_{51}\\ \hline
g_{02}       \\ 0     \\ 0     \\ g_{52}\\ \hline
g_{03}+g_{14}\\ g_{24}\\ g_{34}\\ g_{44}+g_{53}
\end{array}\right].
\end{gather*}
\end{figure}

\section{Matrix structure}
The structure of the matrix arising from the discrete Poisson 
problem~\eqref{eq: 5-pt Poisson} can be understood more easily with the help of 
the following concept.

\begin{definition}
Given matrices
$\boldsymbol{A}\in\mathbb{R}^{M\times N}$~and 
$\boldsymbol{B}\in\mathbb{R}^{P\times Q}$, the \emph{Kronecker product} 
$\boldsymbol{A}\otimes\boldsymbol{B}\in\mathbb{R}^{(MP)\times(NQ)}$ 
is the $M\times N$ block matrix whose $ij$-block equals $a_{ji}\boldsymbol{B}$,
that is,
\[
\boldsymbol{A}\otimes\boldsymbol{B}=\begin{bmatrix}
a_{11}\boldsymbol{B}&a_{12}\boldsymbol{B}&\cdots&a_{1N}\boldsymbol{B}\\
a_{21}\boldsymbol{B}&a_{22}\boldsymbol{B}&\cdots&a_{2N}\boldsymbol{B}\\
              \vdots&              \vdots&\ddots&              \vdots\\
a_{M1}\boldsymbol{B}&a_{M2}\boldsymbol{B}&\cdots&a_{MN}\boldsymbol{B}
\end{bmatrix}.
\]
\end{definition}

\begin{example}\label{example: Kronecker product}
If
\[
\boldsymbol{A}=\begin{bmatrix}1&3\\ 2&4\end{bmatrix}
\quad\text{and}\quad
\boldsymbol{B}=\begin{bmatrix}1&0&-3\\ -2&1&-1\end{bmatrix}
\]
then
\[
\boldsymbol{A}\otimes\boldsymbol{B}
=\begin{bmatrix}
 \boldsymbol{B}&3\boldsymbol{B}\\
2\boldsymbol{B}&4\boldsymbol{B} 
\end{bmatrix}
=\left[\begin{array}{ccc|ccc}
 1& 0&-3& 3& 0&-9\\
-2& 1&-1&-6& 3&-3\\             
\hline
 2& 0&-6& 4& 0&-12\\
-4& 2&-2&-8& 4&-4
\end{array}\right].
\]
\end{example}

The next theorem gives a key relation between the Kronecker product and the 
ordinary matrix product.

\begin{theorem}\label{thm: Kronecker A B C D}
If $\boldsymbol{A}\in\mathbb{R}^{M\times N}$,
$\boldsymbol{B}\in\mathbb{R}^{P\times Q}$,
$\boldsymbol{C}\in\mathbb{R}^{N\times R}$~and
$\boldsymbol{D}\in\mathbb{R}^{Q\times S}$, then
\[
(\boldsymbol{A}\otimes\boldsymbol{B})(\boldsymbol{C}\otimes\boldsymbol{D})
=(\boldsymbol{A}\boldsymbol{C})\otimes(\boldsymbol{B}\boldsymbol{D}).
\]
\end{theorem}
\begin{proof}
The $ij$-block of 
$(\boldsymbol{A}\otimes\boldsymbol{B})(\boldsymbol{C}\otimes\boldsymbol{D})$ 
equals
\[
\sum_{k=1}^N(a_{ik}\boldsymbol{B})(c_{kj}\boldsymbol{D})
    =\biggl(\sum_{k=1}^Na_{ik}c_{kj}\biggr)(\boldsymbol{B}\boldsymbol{D})
    =(\boldsymbol{A}\boldsymbol{C})_{ij}(\boldsymbol{B}\boldsymbol{D}),
\]
which is also the $ij$-block of
$(\boldsymbol{A}\boldsymbol{C})\otimes(\boldsymbol{B}\boldsymbol{D})$.
\end{proof}

\begin{example}
\newcommand{\bs}[1]{\boldsymbol{#1}}
With $\boldsymbol{A}$~and $\boldsymbol{B}$ as in 
Example~\ref{example: Kronecker product}, and with
\[
\boldsymbol{C}=\begin{bmatrix}1&-1&3\\ 5&0&3\end{bmatrix}
\quad\text{and}\quad
\boldsymbol{D}=\begin{bmatrix}1&0&7\\ 0&-4&1\\ 2&3&9\end{bmatrix},
\]
we have
\[
(\bs{A}\otimes\bs{B})(\bs{C}\otimes\bs{D})
    =\begin{bmatrix}
 \bs{B}&3\bs{B}\\
2\bs{B}&4\bs{B}\end{bmatrix}
\begin{bmatrix}
 \bs{D}&-\bs{D}&3\bs{D}\\
5\bs{D}& \bs{0}&3\bs{D}
\end{bmatrix}
=\begin{bmatrix}
16\bs{B}\bs{D}& -\bs{B}\bs{D}&12\bs{B}\bs{D}\\
22\bs{B}\bs{D}&-2\bs{B}\bs{D}&18\bs{B}\bs{D}
\end{bmatrix}
\]
which is the same matrix as
\[
(\bs{A}\bs{C})\otimes(\bs{B}\bs{D})
=\left(\begin{bmatrix}1&3\\ 2&4\end{bmatrix}
\begin{bmatrix}1&-1&3\\ 5&0&3\end{bmatrix}\right)\otimes(\bs{B}\bs{D})
=\begin{bmatrix}16&-1&12\\ 22&-2&18\end{bmatrix}\otimes(\bs{B}\bs{D}).
\]
\end{example}

The Kronecker product makes sense for vector operands, if we identify an
$N$-dimensional row vector with a $1\times N$~matrix, and identify an 
$N$-dimensional column vector with an $N\times1$~matrix.  Suppose that
\[
\newcommand{\bs}[1]{\boldsymbol{#1}}
\bs{U}=\bs{w}\otimes\bs{v}
    =\begin{bmatrix}w_1\bs{v}\\ w_2\bs{v}\\ \vdots\\ w_{Q-1}\bs{v}\end{bmatrix}
\quad\text{for $\bs{v}\in\mathbb{R}^{P-1}$ and $\bs{w}\in\mathbb{R}^{Q-1}$,}
\]
so that
\[
U_j=U_{p,q}=v_pw_q\quad\text{where $j=p+(q-1)(P-1)$,}
\]
for $1\le p\le P-1$ and $1\le q\le Q-1$.  Define
\[
\boldsymbol{A}_x=\frac{1}{\Delta x^2}\begin{bmatrix}
 2&    -1&      &      &\\
-1&     2&    -1&      &\\
  &\ddots&\ddots&\ddots&\\
  &      &    -1&     2&-1\\
  &      &      &    -1& 2\end{bmatrix}\in\mathbb{R}^{(P-1)\times(P-1)}
\]
and
\[
\boldsymbol{A}_y=\frac{1}{\Delta y^2}\begin{bmatrix}
 2&    -1&      &      &\\
-1&     2&    -1&      &\\
  &\ddots&\ddots&\ddots&\\
  &      &    -1&     2&-1\\
  &      &      &    -1& 2\end{bmatrix}\in\mathbb{R}^{(Q-1)\times(Q-1)},
\]
so that
\begin{equation}\label{eq: delta x y separable}
-\delta_x^2U_{p,q}=(\boldsymbol{A}_x\boldsymbol{v})_pw_q
\quad\text{and}\quad
-\delta_y^2U_{p,q}=v_p(\boldsymbol{A}_y\boldsymbol{w})_q,
\end{equation}
if we set $v_0=0=v_P$ and $w_0=0=w_Q$.  These relations correspond to the case 
when $u(x,y)=v(x)w(y)$ so that $-u_{xx}=(-v'')w$~and $u_{yy}=v(-w'')$.

\begin{theorem}
Let
\[
\boldsymbol{A}\boldsymbol{U}=\boldsymbol{f}+\boldsymbol{g}
\]
be the linear system resulting from the discrete Poisson 
problem~\eqref{eq: 5-pt Poisson} scheme~\eqref{eq: 5-pt Poisson}.  Then,
\[
\boldsymbol{A}=\boldsymbol{I}_y\otimes\boldsymbol{A}_x
    +\boldsymbol{A}_y\otimes\boldsymbol{I}_x,
\]
where $\boldsymbol{I}_x$~and $\boldsymbol{I}_y$ are the identity matrices of 
dimension $P-1$~and $Q-1$, respectively.
\end{theorem}
\begin{proof}
We see from \eqref{eq: delta x y separable}~and 
Theorem~\ref{thm: Kronecker A B C D} that
\begin{align*}
\boldsymbol{A}(\boldsymbol{w}\otimes\boldsymbol{v})
    &=\boldsymbol{w}\otimes(\boldsymbol{A}_x\boldsymbol{v})
    +(\boldsymbol{A}_y\boldsymbol{w})\otimes\boldsymbol{v}\\
    &=\bigl(\boldsymbol{I}_y\boldsymbol{w})
        \otimes(\boldsymbol{A}_x\boldsymbol{v})
    +(\boldsymbol{A}_y\boldsymbol{w})
        \otimes(\boldsymbol{I}_x\boldsymbol{v})\\
    &=(\boldsymbol{I}_y\otimes\boldsymbol{A}_x)
        (\boldsymbol{w}\otimes\boldsymbol{v})
    +(\boldsymbol{A}_y\otimes\boldsymbol{I}_x)
        (\boldsymbol{w}\otimes\boldsymbol{v})\\
    &=\bigl(\boldsymbol{I}_y\otimes\boldsymbol{A}_x
    +\boldsymbol{A}_y\otimes\boldsymbol{I}_x\bigr)
        (\boldsymbol{w}\otimes\boldsymbol{v})
\end{align*}
for every $\boldsymbol{v}\in\mathbb{R}^{P-1}$ and 
$\boldsymbol{w}\in\mathbb{R}^{Q-1}$.
\end{proof}


\section{Band Cholesky factorization}

A matrix~$\boldsymbol{A}=[a_{ij}]$ has \emph{upper bandwidth}~$\beta$ 
if $a_{ij}=0$ whenever $j-i>\beta$, or equivalently if $\boldsymbol{A}$ has 
$\beta$~non-zero superdiagonals.  Similarly, $\boldsymbol{A}$ has \emph{lower 
bandwidth}~$\beta$ if $a_{ij}=0$ whenever $i-j>\beta$, so that there are 
$\beta$~non-zero subdiagonals.  For example, the following matrix has upper 
bandwidth~$2$ and lower bandwidth~$3$:
\[
\begin{bmatrix}
 5& 2&-1& 0& 0& 0& 0\\
 1& 6& 0& 8& 0& 0& 0\\
 2& 3& 9& 1& 8& 0& 0\\
-3& 4& 7& 2& 5& 8& 0\\
 0& 0& 2& 8& 3&-7& 1\\
 0& 0& 4&-5& 6& 9& 3
\end{bmatrix}.
\]
If a matrix is symmetric, then its upper and lower bandwidths must be equal, so 
we refer to both as just the \emph{bandwidth}.

Let $\boldsymbol{A}$ be an $n\times n$ symmetric, positive-definite matrix
with bandwith~$\beta$.  We will seek a \emph{band Cholesky factorization}
\begin{equation}\label{eq: Chol fact}
\boldsymbol{A}=\boldsymbol{R}^T\boldsymbol{R},
\end{equation}
where the $n\times n$ matrix~$\boldsymbol{R}=[r_{ij}]$ is upper triangular with 
upper bandwidth~$\beta$.  By the definition of matrix multiplication,
\[
\bigl(\boldsymbol{R}^T\boldsymbol{R}\bigr)_{ij}
	=\sum_{k=1}^n\bigl(\boldsymbol{R}^T\bigr)_{ik}\boldsymbol{R}_{kj}
	=\sum_{k=1}^n r_{ki}r_{kj}.
\]
Since $\boldsymbol{R}$ is upper triangular, $r_{ki}r_{kj}=0$ if $k>i$~or $k>j$,
that is, if $k>\min(i,j)$, so in fact
\[
\bigl(\boldsymbol{R}^T\boldsymbol{R}\bigr)_{ij}
	=\sum_{k=1}^{\min(i,j)} r_{ki}r_{kj}.
\]
In addition, $\boldsymbol{R}$ has upper bandwidth~$\beta$ so $r_{ki}r_{kj}=0$ 
if $i>k+\beta$~or $j>k+\beta$.  Thus, taking account of symmetry, 
\eqref{eq: Chol fact} is satisfied iff
\[
a_{ij}=\sum_{k=\max(1,j-\beta)}^ir_{ki}r_{kj}
	\quad\text{for $\max(1,j-\beta)\le i\le j\le n$.}
\]
We split the last term off the sum, writing
\[
a_{ij}=r_{ii}r_{ij}+\sum_{k=\max(1,j-\beta)}^{i-1}r_{ki}r_{kj}
	\quad\text{for $\max(1,j-\beta)\le i<j\le n$,}
\]
in the off-diagonal case, and
\[
a_{jj}=r_{jj}^2+\sum_{k=\max(1,j-\beta)}^{j-1}r_{kj}^2
	\quad\text{for $1\le j\le n$.}
\]
in the diagonal case.  Rearranging these equations leads to the formulae
\[
r_{ij}=\frac{1}{r_{ii}}\biggl(a_{ij}-\sum_{k=\max(1,j-\beta)}^{i-1}r_{ki}r_{kj}
	\biggr)\quad\text{for $\max(1,j-\beta)\le i\le j\le n$,}
\]
and
\[
r_{jj}=\sqrt{a_{jj}-\sum_{k=\max(1,j-\beta)}^{j-1}r_{kj}^2}
	\quad\text{for $1\le j\le n$,}
\]
which yield Algorithm~\ref{alg: band Chol}.

\begin{algorithm}
\caption{Compute the band Cholesky factorization \eqref{eq: Chol fact}.}
\label{alg: band Chol}
\begin{algorithmic}
\Require{$\boldsymbol{A}=[a_{ij}]$ is a real, $n\times n$, symmetric 
positive-definite matrix with bandwidth~$\beta$.}
\State
\Function{Factorize}{$\boldsymbol{A}$}
\State Allocate storage for the $n\times n$ Cholesky 
factor~$\boldsymbol{R}=[r_{ij}]$ and initialize to zero.
\For{$j=1:n$}
    \For{$i=\max(1,j-\beta):j$}
        \State $s=0$
        \For{$k=\max(1,j-\beta):i-1$}
            \State $s=s+r_{ki}r_{kj}$
        \EndFor
        \State $r_{ij}=(a_{ij}-s)/r_{ii}$
    \EndFor
    \State $s=0$
    \For{$k=\max(1,j-\beta):j-1$}
        \State $s=s+r_{kj}^2$
    \EndFor
    \If{$a_{jj}\le s$} 
        \State Error: $\boldsymbol{A}$ is not positive-definite.
    \EndIf
    \State $r_{jj}=\sqrt{a_{jj}-s}$
\EndFor
\State\Return{$\boldsymbol{R}$}
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Solve the linear system $\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}$ 
given the band Cholesky factorization~\eqref{eq: Chol fact}.}
\label{alg: band Chol solve}
\begin{algorithmic}
\Require{$\boldsymbol{b}=[b_i]_{i=1}^n$ is the right-hand side vector.}
\Require{$\boldsymbol{R}=[r_{ij}]_{i,j=1}^n$ is the band Cholesky factor 
of~$\boldsymbol{A}$, computed via Algorithm~\ref{alg: band Chol}.}
\State
\Function{Solve}{$\boldsymbol{b}, \boldsymbol{R}$}
\State Allocate storage for $\boldsymbol{x}=[x_i]_{i=1}^n$~and
$\boldsymbol{y}=[y_i]_{i=1}^n$.
\For{$i=1:n$}
    \State $s=b_i$
    \For{$k=\max(1,i-\beta):i-1$}
        \State $s=s-r_{ji}y_j$
    \EndFor
    \State $y_i=s/r_{ii}$
\EndFor
\For{$i=n:-1:1$}
    \State $s=y_i$
    \For{$j=i+1:\min(n,i+\beta)$}
        \State $s=s-r_{ij}x_j$
    \EndFor
    \State $x_i=s/r_{ii}$
\EndFor
\State\Return{$\boldsymbol{x}$}
\EndFunction
\end{algorithmic}
\end{algorithm}

Once the Cholesky factor~$\boldsymbol{R}$ is known, we can solve a linear 
system~$\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}$ by first solving the lower 
triangular system $\boldsymbol{R}^T\boldsymbol{y}=\boldsymbol{b}$, and then 
solving the upper triangular 
system~$\boldsymbol{R}\boldsymbol{x}=\boldsymbol{y}$, so that
\[
\boldsymbol{A}\boldsymbol{x}=\boldsymbol{R}^T\boldsymbol{R}\boldsymbol{x}
	=\boldsymbol{R}^T\boldsymbol{y}=\boldsymbol{b}.
\]
Since $r_{ji}=0$ if $j>i$~or $i-j>s$,
\[
(\boldsymbol{R}^T\boldsymbol{y})_i=\sum_{j=1}^nr_{ji}y_j
	=\sum_{j=\max(1,i-\beta)}^ir_{ji}y_j
	=r_{ii}y_i+\sum_{j=\max(1,i-\beta)}^{i-1}r_{ji}y_j
\]
and so $\boldsymbol{R}^T\boldsymbol{y}=\boldsymbol{b}$ iff
\[
y_i=\frac{1}{r_{ii}}\biggl(b_i-\sum_{j=\max(1,i-\beta)}^{i-1}r_{ji}y_j\biggr)
\quad\text{for $1\le i\le n$.}
\]
Similarly, since $r_{ij}=0$ if $i>j$~or $j-i>\beta$,
\[
(\boldsymbol{R}\boldsymbol{x})_i=\sum_{j=1}^nr_{ij}x_j
	=\sum_{j=i}^{\min(n,i+\beta)}r_{ij}x_j
	=r_{ii}x_i+\sum_{j=i+1}^{\min(n,i+\beta)}r_{ij}x_j
\]
and so $\boldsymbol{R}\boldsymbol{x}=\boldsymbol{y}$ iff
\[
x_i=\frac{1}{r_{ii}}\biggl(y_i-\sum_{j=i+1}^{\min(n,i+\beta)}r_{ij}x_j\biggr)
	\quad\text{for $1\le i\le n$.}
\]
These formula lead to Algorithm~\ref{alg: band Chol solve} for solving 
$\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}$.

\begin{algorithm}
\newcommand{\rb}[1]{r^{\mathrm{band}}_{#1}}
\caption{Compute the band Cholesky factorization \eqref{eq: Chol fact} in 
place using symmetric band storage.}
\label{alg: Chol band storage}
\begin{algorithmic}
\Require{The $(\beta+1)\times n$ 
array~$\boldsymbol{R}_{\mathrm{band}}=[\rb{ij}]$
stores a real, $n\times n$, symmetric positive-definite matrix with 
bandwidth~$\beta$, using the symmetric band storage so
$a_{ij}=\rb{\beta+1+i-j,j}$.}
\State
\Function{Factorize}{$\boldsymbol{R}_{\mathrm{band}}$}
\For{$j=1:n$}
    \For{$i=\max(1,j-\beta):j$}
        \State $s=0$
        \For{$k=\max(1,j-\beta):i-1$}
            \State $s=s+\rb{\beta+1+k-i,i}\rb{\beta+1+k-j,j}$
        \EndFor
        \State $\rb{\beta+1+i-j,j}=(\rb{\beta+1+i-j,j}-s)/\rb{\beta+1,i}$
    \EndFor
    \State $s=0$
    \For{$k=\max(1,j-\beta):j-1$}
        \State $s=s+(\rb{\beta+1+k-j,j})^2$
    \EndFor
    \If{$\rb{s+1,j}\le s$} 
        \State Error: $\boldsymbol{A}$ is not positive-definite.
    \EndIf
    \State $\rb{\beta+1,j}=\sqrt{\rb{\beta+1,j}-s}$
\EndFor
\State\Return{$\boldsymbol{R}_{\mathrm{band}}$}
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\newcommand{\rb}[1]{r^{\mathrm{band}}_{#1}}
\caption{Solve in place the linear system 
$\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}$ 
given the band Cholesky factorization~\eqref{eq: Chol fact}.}
\label{alg: band Chol solve in place}
\begin{algorithmic}
\Require{$\boldsymbol{x}=[b_i]_{i=1}^n$ stores the right-hand side vector.}
\Require{The $(\beta+1)\times n$ array 
$\boldsymbol{R}_{\mathrm{band}}=[\rb{ij}]$ stores 
the band Cholesky factor~$\boldsymbol{R}=[r_{ij}]$ of~$\boldsymbol{A}$, as 
computed via Algorithm~\ref{alg: band Chol} so that 
$r_{ij}=\rb{\beta+1+i-j,j}$.}
\State
\Function{Solve}{$\boldsymbol{x},\boldsymbol{R}_{\mathrm{band}}$}
\For{$i=1:n$}
    \State $s=x_i$
    \For{$j=\max(1,i-\beta):i-1$}
        \State $s=s-\rb{\beta+1+j-i,i}x_j$
    \EndFor
    \State $x_i=s/\rb{\beta+1,i}$
\EndFor
\For{$i=n:-1:1$}
    \State $s=x_i$
    \For{$j=i+1:\min(n,i+\beta)$}
        \State $s=s-\rb{\beta+1+i-j,j}x_j$
    \EndFor
    \State $x_i=s/\rb{\beta+1,i}$
\EndFor
\State\Return{$\boldsymbol{x}$}
\EndFunction
\end{algorithmic}
\end{algorithm}

When $\beta$ is small compared to~$n$, storing $\boldsymbol{A}$ as an 
$n\times n$~array wastes a lot of space.  A more efficient method is to use 
the \emph{symmetric band storage scheme} that uses a
$(\beta+1)\times n$~matrix 
$\boldsymbol{A}_{\textrm{band}}=[a^{\mathrm{band}}_{ij}]$ to hold the non-zero 
superdiagonals of~$\boldsymbol{A}$ by putting
\begin{equation}\label{eq: symm band storage}
a^{\mathrm{band}}_{\beta+1+i-j,j}=a_{ij}
	\quad\text{for $\max(1,j-\beta)\le i\le j$ and $1\le j\le n$.}
\end{equation}
For example, if $n=9$ and $\beta=3$, then
\[
\boldsymbol{A}_{\mathrm{b}}=\begin{bmatrix}
     *&     *&     *&a_{14}&a_{25}&a_{36}&a_{47}&a_{58}&a_{69}\\
     *&     *&a_{13}&a_{24}&a_{35}&a_{46}&a_{57}&a_{68}&a_{79}\\
     *&a_{12}&a_{23}&a_{34}&a_{45}&a_{56}&a_{67}&a_{78}&a_{89}\\
a_{11}&a_{22}&a_{33}&a_{44}&a_{55}&a_{66}&a_{77}&a_{88}&a_{99}           
\end{bmatrix}.
\]

\section{Parabolic problems in 2D}

\begin{Exercises}

\exercise
Suppose that the matrices $\boldsymbol{A}$~and $\boldsymbol{B}$ have upper 
bandwidth $s_{\boldsymbol{A}}$~and $s_{\boldsymbol{B}}$, respectively, and that 
the number of columns of~$\boldsymbol{A}$ equals the number of rows 
of~$\boldsymbol{B}$.  Show that the matrix 
product~$\boldsymbol{A}\boldsymbol{B}$ has upper 
bandwidth~$\max(s_{\boldsymbol{A}},s_{\boldsymbol{B}})$.  State and prove the 
corresponding result for lower bandwidths.

\exercise
Write down $\boldsymbol{A}_{\mathrm{band}}$ given 
by~\eqref{eq: symm band storage} if
\[
\boldsymbol{A}=\begin{bmatrix}
 5& 1& 7&  &  &\\
 1& 2& 0&-1&  &\\
 7& 0& 8& 2& 3&\\
  &-1& 2& 5& 1& 6\\
  &  & 3& 1& 7& 2\\
  &  &  & 6& 2& 9
\end{bmatrix}.
\]
\begin{ans}
\[
\boldsymbol{A}_{\mathrm{band}}=\begin{bmatrix}
 *& *& 7&-1& 3& 6\\
 *& 1& 0& 2& 1& 2\\
 5& 2& 8& 5& 7& 9                                
\end{bmatrix}
\]

\end{ans}


\end{Exercises}
