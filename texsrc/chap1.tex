\chapter[Finite differences in 1D]{Finite differences for \\ 
stationary problems in 1D}
\label{chap: finite diff 1d}
We begin our study of numerical methods for partial differential equations 
(PDEs) by treating the one-dimensional (1D) case, which means we deal only with 
an \emph{ordinary} differential equation (ODE).  Moreover, in this chapter we 
limit our attention to \emph{two-point boundary-value problems} for 
\emph{second-order} ODEs.  In fact, we will focus mostly on the very simple 
model problem on the interval~$[0,L]$,
\begin{equation}\label{eq: model 1d}
-u''=f(x)\quad\text{for $0<x<L$,}
	\quad\text{with $u(0)=\gamma_0$ and $u(L)=\gamma_L$,}
\end{equation}
in which the \emph{source term}~$f(x)$ and \emph{boundary data} $\gamma_0$, 
$\gamma_L$ are given, and we seek the \emph{unknown solution}~$u(x)$.  The 
simplest physical interpretation of~\eqref{eq: model 1d} is as a steady-state 
heat equation, so that $u(x)$ is the temperature at~$x$ and $f(x)$ gives the 
density of heat sources.

For a constant source term~$f(x)=c$, the solution is given by
\begin{equation}\label{eq: u const f}
u(x)=\frac{1}{L}\bigl((L-x)\gamma_0+x\gamma_L\bigr)+\frac{c}{2}\,x(L-x)
	\quad\text{for $0\le x\le L$;}
\end{equation}
see Exercise~\ref{ex: u const f} and Figure~\ref{fig: bvp 1d f const}. For a 
general~$f$, Exercise~\ref{ex: variation of params} shows that
\begin{equation}\label{eq: model 1d exact soln}
u(x)=\frac{L-x}{L}\biggl(\gamma_0+\int_0^x yf(y)\,dy\biggr)
	+\frac{x}{L}\biggl(\gamma_L+\int_x^L(L-y)f(y)\,dy\biggr)
	\quad\text{for $0\le x\le L$.}
\end{equation}
Our aim in this chapter is to study finite difference methods for computing 
a numerical approximation to the values of~$u$ at a set of grid points in the 
interval~$[0,L]$.

\begin{figure}
\caption{Solutions of the simple two-point boundary-value 
problem~\eqref{eq: model 1d} with $L=1$ and right-hand side~$f(x)=c$, for 
various choices of the constant~$c$.}\label{fig: bvp 1d f const}
\begin{center}
\includegraphics[scale=0.75]{../src/chap1/bvp1d_const_rhs.pdf}
\end{center}
\end{figure}

\section{Second-order central difference}
To derive a finite difference approximation to the second derivative~$u''(x)$, 
we will use Taylor's theorem in the following form.

\begin{theorem}\label{thm: Taylor remainder}
Let $h>0$. If $f$ is $C^{k+1}$ on the closed interval~$[x,x+h]$ then
\begin{equation}\label{eq: Taylor 1}
f(x+h)=\sum_{j=0}^k\frac{1}{j!}\,f^{(j)}(x)\,h^j+(R_kf)(x,h),
\end{equation}
where the remainder term is given by
\begin{equation}\label{eq: Taylor 2}
(R_kf)(x,h)=\frac{1}{k!}\int_x^{x+h}(x+h-y)^kf^{(k+1)}(y)\,dy
\end{equation}
and satisfies
\begin{equation}\label{eq: Taylor 3}
|(R_kf)(x,h)|\le\frac{h^{k+1}}{(k+1)!}\,\max_{x\le y\le x+h}|f^{(k+1)}(y)|.
\end{equation}
Similarly, if $f$ is $C^{k+1}$ on~$[x-h,x]$, then
\[
f(x-h)=\sum_{j=0}^k\frac{(-1)^j}{j!}\,f^{(j)}(x)\,h^j+(R_kf)(x,-h)
\]
where 
\[
(R_kf)(x,-h)=\frac{(-1)^{k+1}}{k!}\int_{x-h}^x(y+h-x)^kf^{(k+1)}(y)\,dy
\]
and
\[
|(R_kf)(x,-h)|\le\frac{h^{k+1}}{(k+1)!}\,\max_{x-h\le y\le x}|f^{(k+1)}(y)|.
\]
\end{theorem}
\begin{proof}
We use induction on~$k$.  If $k=0$, then the formulae 
\eqref{eq: Taylor 1}~and \eqref{eq: Taylor 2} reduce to
\[
f(x+h)=f(x)+(R_0f)(x,h)\quad\text{where}\quad
(R_0f)(x,h)=\int_x^{x+h}f'(y)\,dy,
\]
which follows from the fundamental theorem of calculus.  Let $k\ge0$ and make 
the induction hypothesis that \eqref{eq: Taylor 1} holds with $R_kf$ given 
by~\eqref{eq: Taylor 2}.  Integrating by parts, we have
\begin{align*}
(R_kf)(x,h)&=\biggl[
	-\frac{(x+h-y)^{k+1}}{(k+1)!}\,f^{(k+1)}(y)\biggr]_{y=x}^{x+h}
	+\int_x^{x+h}\frac{(x+h-y)^{k+1}}{(k+1)!}\,f^{(k+2)}(y)\,dy\\
	&=\frac{h^{k+1}}{(k+1)!}\,f^{(k+1)}(x)+(R_{k+1}f)(x,y),
\end{align*}
implying that \eqref{eq: Taylor 1}~and \eqref{eq: Taylor 2} hold with~$k$
replaced by~$k+1$, as required.  The estimate~\eqref{eq: Taylor 3} is an 
immediate consequence of the inequality
\begin{align*}
|(R_kf)(x,y)|&\le\int_x^{x+h}\frac{(x+h-y)^k}{k!}\,|f^{(k+1)}(y)|\,dy\\
	&\le\biggl(\max_{x\le y\le x+h}|f^{(k+1)}(y)|\biggr)
	\int_x^{x+h}\frac{(x+h-y)^k}{k!}\,dy.
\end{align*}

The expansion for~$f(x-y)$ and the bound for~$(R_kf)(x,-h)|$ follow in a 
similar fashion.


\end{proof}


A straight forward manipulation of such Taylor expansions shows that
\[
f''(x)=\frac{f(x+h)-2f(x)+f(x-h)}{h^2}+O(h^2)\quad\text{as $h\to0$;}
\]
more precisely, the following result holds.

\begin{theorem}\label{thm: 2nd central diff}
If $f$ is $C^4$ on the closed interval~$[x-h,x+h]$, then
\[
\biggl|f''(x)-\frac{f(x+h)-2f(x)+f(x-h)}{h^2}\biggr|\le\frac{h^2}{12}
	\max_{x-h\le y\le x+h}|f^{(4)}(y)|.
\]
\end{theorem}
\begin{proof}
Since
\[
f(x+h)=f(x)+f'(x)h+\tfrac12f''(x)h^2+\tfrac{1}{3!}f'''(x)h^3+(R_3f)(x,h)
\]
and
\[
f(x-h)=f(x)-f'(x)h+\tfrac12f''(x)h^2-\tfrac{1}{3!}f'''(x)h^3+(R_3f)(x,-h)
\]
we see that
\[
f(x+h)+f(x-h)=2f(x)+f''(x)h^2+(R_3f)(x,h)+(R_3f)(x,-h).
\]
Thus,
\begin{equation}\label{eq: second diff remainder}
\frac{f(x+h)-2f(x)+f(x-h)}{h^2}-f''(x)=\frac{(R_3f)(x,h)+(R_3f)(x,-h)}{h^2}
\end{equation}
and since
\begin{align*}
|(R_3f)(x,h)+(R_3f)(x,-h)|&\le
\frac{h^4}{4!}\max_{x\le y\le x+h}|f^{(4)}(y)|
+\frac{h^4}{4!}\max_{x-h\le y\le x}|f^{(4)}(y)|\\
	&\le\frac{2h^4}{4!}\,\max_{x-h\le y\le x+h}|f^{(4)}(y)|
\end{align*}
the result follows.
\end{proof}

To set up a finite difference scheme for~\eqref{eq: model 1d}, we choose a 
positive integer~$P$ and define a uniform grid on~$[0,L]$,
\[
x_p=p\,\Delta x\quad\text{for $0\le p\le P$,}
	\quad\text{where $\Delta x=\frac{L}{P}$.}
\]
In this way, we divide $[0,L]$ into $P$~subintervals, namely $[x_{p-1},x_p]$
for~$1\le p\le P$, each of length~$\Delta x$.  The finite difference solution 
consists of $P+1$ numbers $U_0$, $U_1$, \dots, $U_P$ that approximate $u(x)$ at 
the $P+1$ grid points, that is,
\[
U_p\approx u(x_p)\quad\text{for $0\le p\le P$.}
\]
Noting that $x_{p\pm1}=x_p\pm\Delta x$, we have
\begin{equation}\label{eq: u'' approx}
\frac{u(x_{p+1})-2u(x_p)+u(x_{p-1})}{\Delta x^2}
	=\frac{u(x_p+\Delta x)-2u(x_p)+u(x_p-\Delta x)}{\Delta x^2}
	\approx u''(x_p)
\end{equation}
which suggests the following discrete approximation to the ODE $-u''=f(x)$,
\begin{equation}\label{eq: model 1d discrete}
-\frac{U_{p+1}-2U_p+U_{p-1}}{\Delta x^2}=f(x_p)\quad\text{for $1\le p\le P-1$.}
\end{equation}
We can satisfy the boundary conditions $u(x_0)=u(0)=\gamma_0$ and 
$u(x_P)=u(L)=\gamma_L$ exactly, by putting
\[
U_0=\gamma_0\quad\text{and}\quad U_P=\gamma_L.
\]
When~$p=1$ in~\eqref{eq: model 1d discrete} we move $U_0=\gamma_0$ to the 
right-hand side, 
\[
\frac{2U_1-U_2}{\Delta x^2}=f(x_1)+\frac{\gamma_0}{\Delta x^2},
\]
and similarly when $p=P-1$ we move $U_P=\gamma_L$ to the right-hand side,
\[
\frac{-U_{P-2}+2U_{P-1}}{\Delta x^2}=f(x_{P-1})+\frac{\gamma_L}{\Delta x^2}.
\]

\begin{figure}
\caption{Comparison of exact solution and its finite difference approximation
from Example~\ref{example: bvp1d example}.}\label{fig: bvp1d example}
\begin{center}
\includegraphics[scale=0.75]{../src/chap1/bvp1d_example.pdf}
\end{center}
\end{figure}

\begin{example}\label{example: bvp1d example}
If $P=6$ then the finite difference scheme results in a $5\times5$ 
linear system
\begin{equation}\label{eq: model 1d linear system}
\frac{1}{\Delta x^2}\begin{bmatrix}
 2&-1&  & &\\
-1& 2&-1& &\\
  &-1& 2&-1&\\
  &  &-1& 2&-1\\
  &  &  &-1& 2
\end{bmatrix}
\begin{bmatrix}U_1\\ U_2\\ U_3\\ U_4\\ U_5\end{bmatrix}
=\begin{bmatrix}f_1\\ f_2\\ f_3\\ f_4\\ f_5 \end{bmatrix}
+\frac{1}{\Delta x^2}
\begin{bmatrix}\gamma_0\\ 0\\ 0 \\ 0\\ \gamma_L \end{bmatrix},
\end{equation}
where, for brevity, we have written $f_p=f(x_p)$.  Also, the zero entries in 
the coefficient matrix are left blank.  The problem
\[
-u''=5e^{-x}\quad\text{for $0\le x\le2$,}
    \quad\text{with $u(0)=-1$ and $u(2)=5/2$,}
\]
has the solution
\[
u(x)=A+Bx+5e^{-x}\quad
\text{where $A=\gamma_0+5$ and $B=(\gamma_L-A-5e^{-L})$.}
\]
Figure~\ref{fig: bvp1d example} shows that $U_p$ does in fact provide a 
reasonable approximation to~$u(x_p)$ in this case.
\end{example}

Several questions arise naturally with regard to the above numerical method.
\begin{enumerate}
\item Does the linear system arising from the finite difference approximation 
always have a unique solution?
\item If so, what is an efficient way to compute the $U_p$?
\item How accurate is the approximation~$U_p\approx u(x_p)$?
\item Does the error $U_p-u(x_p)$ tend to zero if~$\Delta x\to0$, and if so how 
rapidly?
\end{enumerate}
The following sections will address these concerns.

\section{Symmetric tridiagonal linear systems}
\label{sec: sym tridiagonal}
How can we solve a symmetric, tridiagonal linear system such as the one
\eqref{eq: model 1d linear system} arising from the finite difference 
scheme~\eqref{eq: model 1d discrete}?  To discuss this problem, consider a 
$5\times5$ matrix of the form
\begin{equation}\label{eq: A symm tridiagonal}
\boldsymbol{A}=\begin{bmatrix}
\alpha_1& \beta_1&        &        &\\
 \beta_1&\alpha_1& \beta_2&        &\\
        & \beta_2&\alpha_3&\beta_3 &\\
        &        & \beta_3&\alpha_4&\beta_4\\
        &        &        & \beta_4&\alpha_5
\end{bmatrix}.
\end{equation}
A standard algorithm involves computing $5\times5$~matrices 
$\boldsymbol{L}$~and $\boldsymbol{D}$ of the form
\[
\boldsymbol{L}=\begin{bmatrix}
     1&      &      &      &\\
\ell_1&     1&      &      &\\
      &\ell_2&     1&      &\\
      &      &\ell_3&     1&\\
      &      &      &\ell_4&1
  \end{bmatrix}
\quad\text{and}\quad
\boldsymbol{D}=\begin{bmatrix}
d_1&   &   &   &\\
   &d_2&   &   &\\
   &   &d_3&   &\\
   &   &   &d_4&\\
   &   &   &   &d_5
  \end{bmatrix}
\]
having the property that
\begin{equation}\label{eq: L D LT}
\boldsymbol{A}=\boldsymbol{L}\boldsymbol{D}\boldsymbol{L}^\top.
\end{equation}
Given a right-hand side vector~$\boldsymbol{b}$, we can solve the linear system
$\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}$ by solving in sequence the three
linear systems
\begin{equation}\label{eq: LDLT systems}
\boldsymbol{L}\boldsymbol{z}=\boldsymbol{b},\qquad
\boldsymbol{D}\boldsymbol{y}=\boldsymbol{z},\qquad
\boldsymbol{L}^\top\boldsymbol{x}=\boldsymbol{y},
\end{equation}
because it will then follow that
\[
\boldsymbol{A}\boldsymbol{x}
    =\boldsymbol{L}\boldsymbol{D}\boldsymbol{L}^T\boldsymbol{x}
    =\boldsymbol{L}\boldsymbol{D}\boldsymbol{y}
    =\boldsymbol{L}\boldsymbol{z}=\boldsymbol{b}.
\]
Since $\boldsymbol{L}$ is \emph{lower triangular} and $\boldsymbol{D}$ is 
\emph{diagonal}, we can easily compute first $\boldsymbol{z}$, then 
$\boldsymbol{y}$ and finally $\boldsymbol{x}$.  To see how, we write out the 
equations in the $5\times5$~case:
\begin{align*}
      z_1    &=b_1,& d_1y_1&=z_1,& x_1+\ell_1 x_2&=y_1,\\
\ell_1z_1+z_2&=b_2,& d_2y_2&=z_2,& x_2+\ell_2 x_3&=y_2,\\
\ell_2z_2+z_3&=b_3,& d_3y_3&=z_3,& x_3+\ell_3 x_4&=y_3,\\
\ell_3z_3+z_4&=b_4,& d_4y_4&=z_4,& x_4+\ell_4 x_5&=y_4,\\
\ell_4z_4+z_5&=b_5,& d_5y_5&=z_5,& x_5           &=y_5.
\end{align*}
Hence, the steps of the computation are as follows:
\begin{align*}
z_1&=b_1,           & y_1&=z_1/d_1,& x_5&=y_5,\\
z_2&=b_2-\ell_1 z_1,& y_2&=z_2/d_2,& x_4&=y_4-\ell_4x_5,\\
z_3&=b_3-\ell_2 z_2,& y_3&=z_3/d_3,& x_3&=y_3-\ell_3x_4,\\
z_4&=b_4-\ell_3 z_3,& y_4&=z_4/d_4,& x_2&=y_2-\ell_2x_3,\\
z_5&=b_5-\ell_4 z_4,& y_5&=z_5/d_5,& x_1&=y_1-\ell_1x_2.\\
\end{align*}
Matrix multiplication gives
\[
\boldsymbol{L}\boldsymbol{D}\boldsymbol{L}^\top=\begin{bmatrix}
      d_1&      \ell_1d_1&               &               &         \\
\ell_1d_1&d_2+\ell_1^2d_1&      \ell_2d_2&               &         \\
         &      \ell_2d_2&d_3+\ell_2^2d_2&      \ell_3d_3&         \\
         &               &      \ell_3d_3&d_4+\ell_3^2d_3&\ell_4d_4\\
         &               &               &      \ell_4d_4&d_5+\ell_4^2d_4
\end{bmatrix},
\]
so the factorization~\eqref{eq: L D LT} requires that
\begin{align*}
     \alpha_1&=d_1,      &        &                 &&&&&&\\
      \beta_1&=\ell_1d_1,&\alpha_2&=d_2+\ell_1^2d_1,&&&&&&\\
    &&\beta_2&=\ell_2d_2,&\alpha_3&=d_3+\ell_2^2d_2,&&&&\\
  &&&&\beta_3&=\ell_3d_3,&\alpha_4&=d_4+\ell_3^2d_3,&&\\
&&&&&&\beta_4&=\ell_4d_4,&\alpha_5&=d_5+\ell_4^2d_4.
\end{align*}
Thus, the steps in computing the entries of $\boldsymbol{L}$~and 
$\boldsymbol{D}$ are as follows:
\begin{align*}
         d_1&=\alpha_1,   &   &                      &&&&&&\\
      \ell_1&=\beta_1/d_1,&d_2&=\alpha_2-\ell_1^2d_1,&&&&&&\\
    &&\ell_2&=\beta_2/d_2,&d_3&=\alpha_3-\ell_2^2d_2,&&&&\\
  &&&&\ell_3&=\beta_3/d_3,&d_4&=\alpha_4-\ell_3^2d_3,&&\\
&&&&&&\ell_4&=\beta_4/d_4,&d_5&=\alpha_5-\ell_4^2d_4.
\end{align*}
In the general $n\times n$~case,
Algorithm~\ref{alg: LDLT} computes the factorization~\eqref{eq: L D LT}
and Algorithm~\ref{alg: solve symmetric tridiagonal} 
uses this factorization to solve the linear 
system~$\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}$. 

The derivations above show that if the factorization \eqref{alg: LDLT} exists,
then it is unique, that is, $\boldsymbol{L}$ and $\boldsymbol{D}$ are uniquely 
determined by~$\boldsymbol{A}$.  Unfortunately,
Algorithm~\ref{alg: LDLT} can break down because one of the $d_j$ is zero, as 
the following example shows.

\begin{example}
The $3\times3$ matrix
\[
\boldsymbol{A}=\begin{bmatrix}1&1&0\\1&1&2\\ 0&2&0 \end{bmatrix}
\]
is non-singular with inverse
\[
\boldsymbol{A}^{-1}=\begin{bmatrix}1&0&-1/2\\ 0&0&1/2\\ -1/2&1/2&0\end{bmatrix},
\]
but Algorithm~\ref{alg: LDLT} produces
\[
d_1=1,\quad \ell_1=1,\quad d_2=0,
\]
and then breaks down because $\ell_2=2/0$.
\end{example}

However, the following result can be shown.

\begin{theorem}
If the $n\times n$, symmetric, tridiagonal matrix $\boldsymbol{A}$ is 
positive-definite, then the factorization~\eqref{eq: L D LT} exists and $d_j>0$ 
for $1\le j\le n$.
\end{theorem}

\begin{algorithm}
\caption{Compute the factorization \eqref{eq: L D LT} for a symmetric, 
tridiagonal matrix $\boldsymbol{A}$.}
\label{alg: LDLT}
\begin{algorithmic}
\Require{$\boldsymbol{\alpha}=[\alpha_1,\alpha_2,\ldots,\alpha_n]$ is the main 
diagonal of $\boldsymbol{A}$.}
\Require{$\boldsymbol{\beta}=[\beta_1,\beta_2,\ldots,\beta_{n-1}]$ is the 
off-diagonal of $\boldsymbol{A}$.}
\Statex
\Function{Factorize}{$\boldsymbol{\alpha}, \boldsymbol{\beta}$}
\State Allocate storage for $\boldsymbol{d}=[d_1,d_2,\ldots, d_n]$ and
$\boldsymbol{\ell}=[\ell_1,\ell_2,\ldots,\ell_{n-1}]$
\State $d_1=\alpha_1$
\For{$j=1:n-1$}
\State $\ell_j=\beta_j/d_j$
\State $d_{j+1}=\alpha_{j+1}-\ell_j^2d_j$
\EndFor
\State\Return{$\boldsymbol{d}$, $\boldsymbol{\ell}$}
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Solve a symmetric, tridiagonal linear system 
$\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}$ given the 
factorization~\eqref{eq: L D LT}.}
\label{alg: solve symmetric tridiagonal}
\begin{algorithmic}
\Require{$\boldsymbol{b}=[b_1,b_2,\ldots,b_n]$ the right-hand side vector.}
\Require{$\boldsymbol{d}=[d_1, d_2, \ldots, d_n]$ the main diagonal of 
$\boldsymbol{D}$.}
\Require{$\boldsymbol{\ell}=[\ell_1,\ell_2,\ldots,\ell_{n-1}]$ the first 
sub-diagonal of~$\boldsymbol{L}$.}
\Statex
\Function{Solve}{$\boldsymbol{b},\boldsymbol{d},\boldsymbol{\ell}$}
\State Allocate storage for $\boldsymbol{x}=[x_1,x_2,\ldots,x_n]$,
$\boldsymbol{y}=[y_1,y_2,\ldots,y_n]$ and $\boldsymbol{z}=[z_1,z_2,\ldots,z_n]$.
\State $z_1=b_1$
\For{$j=1:n-1$}
\State $z_{j+1}=b_{j+1}-\ell_jz_j$
\EndFor
\For{$j=1:n$}
\State $y_j=z_j/d_j$
\EndFor
\State $x_n=y_n$
\For{$j=n-1:-1:1$}
\State $x_j=y_j-\ell_jx_{j+1}$
\EndFor
\State\Return{$\boldsymbol{x}$}
\EndFunction
\end{algorithmic}
\end{algorithm}

Once $d_j$ has been computed, the value of $\alpha_j$ is never used in 
subsequent steps of Algorithm~\ref{alg: LDLT}. Similarly, once $\ell_j$ has been 
computed, the value of~$\beta_j$ is never used subsequently.  It is therefore 
possible to economize on storage by computing the factorization 
\emph{in place}, that is, overwriting $\alpha_j$ with~$d_j$,
and likewise overwriting $\beta_j$ with~$\ell_j$, as shown in 
Algorithm~\ref{alg: LDLT in place}.  The solution of the triangular and 
diagonal linear systems can also be performed in place, as shown in 
Algorithm~\ref{alg: solve symmetric tridiagonal in place}.  Here, we have 
followed the convention from Julia that an exclamation mark~! is appended to 
the name of any function that modifies at least one of its arguments.  Also, 
the symbol~$\gets$ in this context means ``is overwritten with''.

\begin{algorithm}
\caption{Compute the factorization \eqref{eq: L D LT} in place.}
\label{alg: LDLT in place}
\begin{algorithmic}
\Require{$\boldsymbol{d}=[\alpha_1,\alpha_2,\ldots,\alpha_n]$ holds the main
diagonal of $\boldsymbol{A}$.}
\Require{$\boldsymbol{\ell}=[\beta_1,\beta_2,\ldots,\beta_{n-1}]$ holds the 
off-diagonal of $\boldsymbol{A}$.}
\Statex
\Function{Factorize!}{$\boldsymbol{d}$, $\boldsymbol{\ell}$}
\For{$j=1:n-1$}
\State $\ell_j\gets\ell_j/d_j$
\State $d_{j+1}\gets d_{j+1}-\ell_j^2d_j$
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Solve a symmetric, tridiagonal linear system 
$\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}$ in place.}
\label{alg: solve symmetric tridiagonal in place}
\begin{algorithmic}
\Require{$\boldsymbol{x}=[b_1,b_2,\ldots,b_n]$ holds the right-hand side 
vector.}
\Require{$\boldsymbol{d}=[d_1, d_2, \ldots, d_n]$ holds the main diagonal of 
$\boldsymbol{D}$.}
\Require{$\boldsymbol{\ell}=[\ell_1,\ell_2,\ldots,\ell_{n-1}]$ holds the first 
sub-diagonal of~$\boldsymbol{L}$.}
\Statex
\Function{Solve!}{$\boldsymbol{x},\boldsymbol{d},\boldsymbol{\ell}$}
\For{$j=1:n-1$}
    \State $x_{j+1}\gets x_{j+1}-\ell_jx_j$
\EndFor
\For{$j=1:n$}
    \State $x_j\gets x_j/d_j$
\EndFor
\For{$j=n-1:-1:1$}
    \State $x_j\gets x_j-\ell_jx_{j+1}$
\EndFor
\EndFunction
\end{algorithmic}
\end{algorithm}

Counting the numbers of arithmetic operations in Algorithms
\ref{alg: LDLT}~ and \ref{alg: solve symmetric tridiagonal} yields
\cref{tab: LDLT flops}.  If we use the in-place versions, 
Algorithms \ref{alg: LDLT in place}~and 
\ref{alg: solve symmetric tridiagonal in place}, then we have to store only
$3n-1$ floating-point numbers.  Thus, the overall computational cost is 
$O(n)$~operations and $O(n)$~storage, implying that the computational cost of
solving the finite difference equations~\eqref{eq: model 1d discrete} is 
$O(P)$~operations and $O(P)$~storage.  We therefore expect that the runtime and 
memory requirements of a program to compute~$\boldsymbol{U}$ will scale 
linearly with~$P$.

\begin{table}
\caption{Operation counts for solving a symmetric positive-definite, 
tridiagonal linear system.}\label{tab: LDLT flops}
\begin{center}
\begin{tabular}{c|c|c}
&Algorithm~\ref{alg: LDLT}
&Algorithm~\ref{alg: solve symmetric 
tridiagonal}\\
\hline
additions/subtractions&   $n-1$&$2(n-1)$\\
       multiplications&$2(n-1)$&$2(n-1)$\\
             divisions&   $n-1$&$n$
\end{tabular}
\end{center}
\end{table}


\section{General two-point boundary-value problem}
\label{sec: gen two-point bvp}

Now consider a general second-order linear differential operator,
\[
\mathcal{L}u=-a(x)u''+b(x)u'+c(x)u,
\]
where, for simplicity, we will assume that the coefficients $a$, $b$ and 
$c$ are continuous on~$[0,L]$, and that the \emph{leading coefficient}~$a$ is 
\emph{strictly positive} on~$[0,L]$, so there is a constant~$a_{\min}$ such that
\begin{equation}\label{eq: ellipticity 1d}
a(x)\ge a_{\min}>0\quad\text{for $0\le x\le L$.}
\end{equation}
The general \emph{two-point boundary-value problem} is to find $u=u(x)$ 
satisfying
\begin{equation}\label{eq: two-point bvp}
\begin{aligned}
\mathcal{L}u&=f(x)&&\text{for $0<x<L$,}\\
\alpha_0u'+\beta_0u&=\gamma_0&&\text{at $x=0$,}\\
\alpha_Lu'+\beta_Lu&=\gamma_L&&\text{at $x=L$,}
\end{aligned}
\end{equation}
where, for the boundary conditions to make sense, we assume that at least one 
of $\alpha_0$~and $\beta_0$ is not zero, and likewise at least one of  
$\alpha_L$~and $\beta_L$ is not zero.  For simplicity, we will also assume that 
the function~$f$ is continuous on~$[0,L]$.  The simple 
problem~\eqref{eq: model 1d} is just the special case
\[
a(x)=1,\quad b(x)=0,\quad 
c(x)=0,\quad\alpha_0=0,\quad\beta_0=1,\quad\alpha_L=0,\quad\beta_L=1.
\]

To construct a finite difference scheme for~\eqref{eq: two-point bvp}, we need 
to approximate the first derivative~$u'$.  Once again, we apply Taylor 
expansions, this time showing that
\[
\frac{f(x+h)-f(x-h)}{2h}=f'(x)+O(h^2)\quad\text{as $h\to0$.}
\]
More precisely, the following holds.

\begin{theorem}\label{thm: first central diff}
If $f$ is $C^3$ on the closed interval~$[x-h,x+h]$, then
\[
\biggl|f'(x)-\frac{f(x+h)-f(x-h)}{2h}\biggr|
	\le\frac{h^2}{6}\,\max_{x-h\le y\le x+h}|f'''(y)|.
\]
\end{theorem}
\begin{proof}
Since
\[
f(x+h)=f(x)+f'(x)h+\tfrac12f''(x)h^2+(R_2f)(x,h)
\]
and
\[
f(x-h)=f(x)-f'(x)h+\tfrac12f''(x)h^2+(R_2f)(x,-h),
\]
we have
\[
f(x+h)-f(x-h)=2f'(x)h+(R_2f)(x,h)-(R_2f)(x,-h).
\]
Thus,
\[
\frac{f(x+h)-f(x-h)}{2h}-f'(x)=\frac{(R_2f)(x,h)-(R_2f)(x,-h)}{2h},
\]
and since
\begin{align*}
\bigl|(R_2f)(x,h)-(R_2f)(x,-h)\bigr|
	&\le\frac{h^3}{3!}\max_{x\le y\le x+h}|f'''(y)|
 	   +\frac{h^3}{3!}\max_{x-h\le y\le x}|f'''(y)|\\
	&\le\frac{2h^3}{3!}\max_{x-h\le y\le x+h}|f'''(y)|,
\end{align*}
the result follows at once.
\end{proof}

The approximations \eqref{eq: u'' approx}~and
\begin{equation}\label{eq: u' approx}
\frac{u(x_{p+1})-u(x_{p-1})}{2\,\Delta x}
	=\frac{u(x_p+\Delta x)-u(x_p-\Delta x)}{2\,\Delta x}
	\approx u'(x_p)
\end{equation}
suggest the following discrete approximation to~$(\mathcal{L}u)(x_p)$, 
\begin{equation}\label{eq: Lu Delta x}
(\mathcal{L}_{\Delta x}U)_p=
-a_p\,\frac{U_{p+1}-2U_p+U_{p-1}}{\Delta x^2}
	+b_p\,\frac{U_{p+1}-U_{p-1}}{2\,\Delta x}+c_pU_p,
\end{equation}
where we have used the abbreviations $a_p=a(x_p)$, $b_p=b(x_p)$~and 
$c_p=c(x_p)$.  The central difference approximation to~$u'$ can also be used 
in the boundary conditions by allowing ``ghost'' grid points $x_{-1}=-\Delta 
x$~and $x_{P+1}=L+\Delta x$ lying just outside the interval~$[0,L]$, so that
\begin{equation}\label{eq: bc ghost points}
u'(0)\approx\frac{U_1-U_{-1}}{2\,\Delta x}
\quad\text{and}\quad
u'(L)\approx\frac{U_{P+1}-U_{P-1}}{2\,\Delta x}.
\end{equation}
Four cases can occur.
\begin{enumerate}
\item If $\alpha_0=0$ and $\alpha_L=0$, then we require
\[
(\mathcal{L}_{\Delta x}U)_p=f_p\quad\text{for $1\le p\le P-1$,}\quad
\text{with $\beta_0U_0=\gamma_0$ and $\beta_LU_P=\gamma_L$.}
\]
Eliminating $U_0=\gamma_0/\beta_0$~and $U_P=\gamma_L/\beta_L$ leads to a linear 
system for $U_1$, $U_2$, \dots, $U_{P-1}$.
\item If $\alpha_0\ne0$ and $\alpha_L=0$, then we require
\[
(\mathcal{L}_{\Delta x}U)_p=f_p\quad\text{for $0\le p\le P-1$,}\quad
\text{with $\alpha_0\,\frac{U_1-U_{-1}}{2\,\Delta x}+\beta_0U_0=\gamma_0$
and $\beta_LU_P=\gamma_L$.}
\]
Eliminating $U_{-1}=U_1+2\,\Delta x\,(\beta_0U_0-\gamma_0)/\alpha_0$ and 
$U_P=\gamma_L/\beta_L$ leads to a linear system for $U_0$, $U_1$, \dots, 
$U_{P-1}$.
\item If $\alpha_0=0$ and $\alpha_L\ne0$, then we require
\[
(\mathcal{L}_{\Delta x}U)_p=f_p\quad\text{for $1\le p\le P$,}\quad
\text{with $\beta_0U_0=\gamma_0$ and
$\alpha_L\,\frac{U_{P+1}-U_{P-1}}{2\,\Delta x}+\beta_LU_P=\gamma_L$.}
\]
Eliminating $U_0=\gamma_0/\beta_0$ and
$U_{P+1}=U_{P-1}+2\,\Delta x(\gamma_L-\beta_LU_P)/\alpha_L$ leads to a linear 
system for $U_1$, $U_2$, \dots, $U_P$.
\item If $\alpha_0\ne0$ and $\alpha_L\ne0$, then we require
\[
(\mathcal{L}_{\Delta x}U)_p=f_p\quad\text{for $0\le p\le P$,}
\]
with
\[
\alpha_0\,\frac{U_1-U_{-1}}{2\,\Delta x}+\beta_0U_0=\gamma_0
\quad\text{and}\quad
\alpha_L\,\frac{U_{P+1}-U_{P-1}}{2\,\Delta x}+\beta_LU_P=\gamma_L.
\]
Eliminating $U_{-1}$~and $U_{P+1}$ leads to a linear system for
$U_0$, $U_1$, \dots, $U_P$.
\end{enumerate}

In case~1, the first equation is
\[
-a_1\,\frac{U_2-2U_1+U_0}{\Delta x^2}+b_1\,\frac{U_2-U_0}{2\,\Delta x}
	+c_1U_1=f_1
\]
so $U_0=\gamma_0/\beta_0$ gives
\[
a_1\,\frac{2U_1-U_2}{\Delta x^2}+b_1\,\frac{U_2}{2\,\Delta x}+c_1U_1
	=f_1+\biggl(\frac{a_1}{\Delta x^2}+\frac{b_1}{2\,\Delta x}\biggr)
	\frac{\gamma_0}{\beta_0}.
\]
Similarly, the last equation is
\[
-a_{P-1}\,\frac{U_P-2U_{P-1}+U_{P-2}}{\Delta x^2}
	+b_{P-1}\,\frac{U_P-U_{P-2}}{2\,\Delta x}+c_{P-1}U_{P-1}=f_{P-1},
\]
so $U_P=\gamma_L/\beta_L$ gives
\[
a_{P-1}\,\frac{-U_{P-2}+2U_{P-1}}{\Delta x^2}
	-b_{P-1}\,\frac{U_{P-2}}{2\,\Delta x}+c_{P-1}U_{P-1}
	=f_{P-1}+\biggl(\frac{a_{P-1}}{\Delta x^2}-\frac{b_{P-1}}{2\,\Delta x}
	\biggr)\,\frac{\gamma_L}{\beta_L}.
\]
For example, if $P=6$ we obtain a $5\times5$ linear system
\begin{equation}\label{eq: finite diff matrix 1d}
\boldsymbol{A}\boldsymbol{U}+\boldsymbol{B}\boldsymbol{U}
	+\boldsymbol{C}\boldsymbol{U}=\boldsymbol{f}+\boldsymbol{g},
\end{equation}
where
\begin{gather*}
\boldsymbol{A}=\frac{1}{\Delta x^2}\begin{bmatrix}
2a_1&-a_1&    &    &    \\
-a_2&2a_2&-a_2&    &    \\
    &-a_3&2a_3&-a_3&    \\
    &    &-a_4&2a_4&-a_4\\
    &    &    &-a_5&2a_5\\
               \end{bmatrix},\qquad
\boldsymbol{B}=\frac{1}{2\,\Delta x}\begin{bmatrix}
   0& b_1&    &    &    \\
-b_2&   0& b_2&    &    \\
    &-b_3&   0& b_3&    \\
    &    &-b_4&   0& b_4\\
    &    &    &-b_5&   0\end{bmatrix},\\
\boldsymbol{C}=\begin{bmatrix}
c_1&   &   &   &   \\
   &c_2&   &   &   \\
   &   &c_3&   &   \\
   &   &   &c_4&   \\
   &   &   &   &c_5
\end{bmatrix},\quad
\boldsymbol{U}=\begin{bmatrix} U_1\\ U_2\\ U_3\\ U_4\\ U_5\end{bmatrix},\quad
\boldsymbol{f}=\begin{bmatrix} f_1\\ f_2\\ f_3\\ f_4\\ f_5\end{bmatrix},\quad
\boldsymbol{g}=\begin{bmatrix} g_1\\ 0\\ 0\\ 0\\ g_5 \end{bmatrix},
\end{gather*}
where
\[
g_1=\biggl(\frac{a_1}{\Delta x^2}+\frac{b_1}{2\,\Delta x}\biggr)
	\frac{\gamma_0}{\beta_0}
\quad\text{and}\quad
g_5=\biggl(\frac{a_{P-1}}{\Delta x^2}-\frac{b_{P-1}}{2\,\Delta x}
	\biggr)\,\frac{\gamma_L}{\beta_L}.
\]

In case~4, the first equation is 
\[
-a_0\,\frac{U_1-2U_0+U_{-1}}{\Delta x^2}+b_0\,\frac{U_1-U_{-1}}{2\,\Delta x}
    +c_0U_0=f_0
\]
so $U_{-1}=U_1+2\,\Delta x(\beta_0U_0-\gamma_0)/\alpha_0$ gives
\[
-2a_0\,\frac{U_1-U_0}{\Delta x^2}+c_0U_0
    -\frac{\beta_0(2a_0+b_0\,\Delta x)}{\alpha_0\,\Delta x}\,U_0
    =f_0
    -\frac{2a_0+b_0\,\Delta x}{\alpha_0\,\Delta x}\,\gamma_0.
\]
Similarly, the last equation is
\[
-a_P\,\frac{U_{P+1}-2U_P+U_{P-1}}{\Delta x^2}
    +b_P\,\frac{U_{P+1}-U_{P-1}}{2\,\Delta x}+c_PU_P=f_P,
\]
so $U_{P+1}=U_{P-1}+2\,\Delta x(\gamma_L-\beta_LU_P)/\alpha_L$ gives
\[
-2a_P\,\frac{-U_P+U_{P-1}}{\Delta x^2}+c_PU_P
    +\frac{\beta_L(2a_P-b_P\,\Delta x)}{\alpha_L\,\Delta x}\,U_P
    =f_P+\frac{2a_P-b_P\,\Delta x}{\alpha_L\,\Delta x}\,\gamma_L.
\]
For example, if $P=4$ then we obtain a $5\times5$ linear 
system of the form~\eqref{eq: finite diff matrix 1d} but now
\begin{gather*}
\boldsymbol{A}=\frac{1}{\Delta x^2}\begin{bmatrix}
2a_0(1-\beta_0\,\Delta x/\alpha_0)&-2a_0&    &    &    \\
-a_1&2a_1&-a_1&    &    \\
    &-a_2&2a_2&-a_2&    \\
    &    &-a_3&2a_3&-a_3\\
    &    &    &-2a_4&2a_4(1+\beta_L\,\Delta x/\alpha_L)\\
               \end{bmatrix},\\
\boldsymbol{B}=\frac{1}{2\,\Delta x}\begin{bmatrix}
-2b_0\beta_0\,\Delta x/\alpha_0&    &    &    &    \\
-b_1&   0& b_1&    &    \\
    &-b_2&   0& b_2&    \\
    &    &-b_3&   0& b_3\\
    &    &    &    &-2b_P\beta_L\,\Delta x/\alpha_L\end{bmatrix},\\
\boldsymbol{C}=\begin{bmatrix}
c_0&   &   &   &   \\
   &c_1&   &   &   \\
   &   &c_2&   &   \\
   &   &   &c_3&   \\
   &   &   &   &c_4
\end{bmatrix},\quad
\boldsymbol{U}=\begin{bmatrix} U_0\\ U_1\\ U_2\\ U_3\\ U_4\end{bmatrix},\quad
\boldsymbol{f}=\begin{bmatrix} f_0\\ f_1\\ f_2\\ f_3\\ f_4\end{bmatrix},\quad
\boldsymbol{g}=\begin{bmatrix} g_0\\ 0\\ 0\\ 0\\ g_4 \end{bmatrix},
\end{gather*}
where
\[
g_0=-\frac{2a_0+b_0\,\Delta x}{\alpha_0\,\Delta x}\,\gamma_0
\quad\text{and}\quad
g_4=\frac{2a_4-b_4\,\Delta x}{\alpha_L\,\Delta x}\,\gamma_L.
\]

The procedure in cases 2~and 3 should now be clear.

\section{Maximum principle}
It will be convenient to use the standard notation
\[
\|f\|_\infty=\max_{0\le x\le L}|f(x)|
\]
for the \emph{maximum norm} of any (continuous) function~$f$ over 
the interval~$[0,L]$, and to let
\[
u^+(x)=\max\{u(x),0\}
\quad\text{and}\quad
u^-(x)=\min\{u(x),0\}.  
\]
We begin by proving a \emph{maximum principle} that 
follows from our \emph{ellipticity} assumption~\eqref{eq: ellipticity 1d} and 
simple calculus.

\begin{lemma}\label{lem: Lu<0}
Assume that $u$ is $C^2$ on the open interval~$(0,L)$.
If $c\ge0$ and $\mathcal{L}u<0$ on~$(0,L)$, then $u$ cannot attain a 
non-negative local maximum in~$(0,L)$.
\end{lemma}
\begin{proof}
Suppose for a contradiction that there exist $x_0\in(0,L)$~and $\delta>0$ such
that
\[
u(x_0)\ge0\quad\text{and}\quad
\text{$u(x)\le u(x_0)$ for~$x\in(x_0-\delta,x_0+\delta)\subseteq(0,L)$.}
\]
Since $u$ has an interior local maximum at~$x_0$, it follows that 
$u'(x_0)=0$~and $u''(x_0)\le0$ so 
\[
(\mathcal{L}u)(x_0)=-a(x_0)u''(x_0)+c(x_0)u(x_0)\ge 0,
\]
contradicting our assumption that $\mathcal{L}u<0$ on~$(0,L)$.
\end{proof}

\begin{theorem}\label{thm: max principle 1d}
Assume that $u$ is continuous on the closed interval~$[0,L]$ and is $C^2$ on 
the open interval~$(0,L)$. If $c\ge0$ and $\mathcal{L}u\le0$ on~$(0,L)$, then
\[
u(x)\le\max\{u^+(0),u^+(L)\}\quad\text{for $0<x<L$.}
\]
\end{theorem}
\begin{proof}
Let $\epsilon>0$~and $\mu>0$, and put $w(x)=u(x)+\epsilon e^{\mu x}$.  Since
\[
(\mathcal{L}w)(x)
	=(\mathcal{L}u)(x)+\epsilon\bigl[-a(x)\mu^2+b(x)\mu+c(x)\bigr]e^{\mu x}
\le-\epsilon\bigl[a_{\min}\mu^2-\mu\|b\|_\infty-\|c\|_\infty\bigr]e^{\mu x}
\]
by choosing $\mu$ sufficiently large we can ensure that $\mathcal{L}w<0$
on~$(0,L)$.  By Lemma~\ref{lem: Lu<0}, the function~$w$ cannot attain a 
non-negative local maximum in~$(0,L)$, implying that 
\[
u(x)\le w(x)\le w^+(x)\le\max\{w^+(0),w^+(L)\}
	\le\max\{u^+(0)+\epsilon,u^+(L)+\epsilon e^{\mu L}\}
\]
for~$0<x<L$.  Since this inequality holds for any~$\epsilon>0$, the result 
follows.
\end{proof}

\begin{theorem}\label{thm: max min 1d}
Assume that $u$ is continuous on the closed interval~$[0,L]$ and is $C^2$ on 
the open interval~$(0,L)$. If $c\ge0$ and $\mathcal{L}u=f$ on~$(0,L)$, then
\[
\max_{[0,L]}u\le\max\{u^+(0),u^+(L)\}+\cosh(\mu L/2)\max_{[0,L]}f^+
\]
and
\[
\min_{[0,L]}u\ge\min\{u^-(0),u^-(L)\}+\cosh(\mu L/2)\min_{[0,L]}f^-,
\]
where $\mu=\max\bigl\{1,(1+\|b\|_\infty)/a_{\min}\bigr\}$.
\end{theorem}
\begin{proof}
Let
\[
w(x)=\max\{u^+(0),u^+(L)\}+v(x)\,\max_{[0,L]}f^+
\quad\text{where}\quad
v(x)=\cosh(\mu L/2)-\cosh\mu(x-L/2),
\]
and observe that $v\ge0$ on~$[0,L]$ with
\begin{align*}
(\mathcal{L}v)(x)&=a(x)\mu^2\cosh\mu(x-L/2)
	-b(x)\mu\sinh\mu(x-L/2)\\
	&\qquad{}+c(x)\bigl(\max\{u^+(0),u^+(L)\}+v(x)\big)\\
	&\ge\bigl(a_{\min}\mu^2-\|b\|_\infty\mu\bigr)\cosh\mu(x-L/2)
	\ge(a_{\min}\mu-\|b\|_\infty)\mu\ge1
\end{align*}
for $0<x<L$, so
\[
\mathcal{L}(u-w)=f-c(x)\max\{u^+(0),u^+(L)\}-(\mathcal{L}v)\max_{[0,L]}f^+
	\le0\quad\text{on $(0,L)$.}
\]
By Theorem~\ref{thm: max principle 1d},
\[
u(x)-w(x)\le\max\{(u-w)^+(0),(u-w)^+(L)\}\quad\text{for $0<x<L$,}
\]
and since $v(0)=0=v(L)$ we see that 
\[
(u-w)(0)=u(0)-\max\{u^+(0),u^+(L)\}\le0
\]
and
\[
(u-w)(L)=u(L)-\max\{u^+(0),u^+(L)\}\le0.
\]
Thus, $u-w\le0$ on~$(0,L)$, and therefore 
\[
\max_{[0,L]}u\le\max_{[0,L]}w(x)=w(L/2)=\max\{u^+(0),u^+(L)\}
	+v(L/2)\max_{[0,L]}f, 
\]
proving the first inequality.  
The second follows because $\mathcal{L}(-u)=-f$, $(-u)^+=-u^-$~and
$(-f)^+=-f^-$.
\end{proof}

Now consider the two-point boundary-value problem~\eqref{eq: two-point bvp} in 
the case when $\alpha_0=0=\alpha_L$ and $\beta_0=1=\beta_L$:
\begin{equation}\label{eq: Lu=f Dirichlet}
\mathcal{L}u=f\quad\text{on $(0,L)$,}
	\quad\text{with $u(0)=\gamma_0$ and $u(L)=\gamma_L$.}
\end{equation}
As an immediate consequence of Theorem~\ref{thm: max min 1d} we have the 
following \emph{a priori} estimate, which serves to bound the solution~$u$ in 
terms of the data $f$, $\gamma_0$~and $\gamma_L$.

\begin{theorem}\label{thm: Lu=f apriori infty}
If $c\ge0$ on~$(0,L)$, then any solution~$u$ of~\eqref{eq: Lu=f Dirichlet} 
satisfies
\[
\|u\|_\infty\le\max\{|\gamma_0|,|\gamma_L|\}+\cosh(\mu L/2)\|f\|_\infty.
\]
\end{theorem}

Using Theorem~\ref{thm: Lu=f apriori infty} it can be shown that
the two-point boundary-value problem~\eqref{eq: Lu=f Dirichlet} is 
\emph{well-posed}, that is, 
\begin{enumerate}
\item a unique solution exists for each choice of the data $f$, $\gamma_0$~and 
$\gamma_L$, and 
\item small changes in the data lead to only small changes in the solution.
\end{enumerate}
To explain the second part, suppose that we perturb the data to $\tilde f$, 
$\tilde\gamma_0$ and $\tilde\gamma_L$, and let $\tilde u$ denote the solution 
of the resulting perturbed problem,
\[
\mathcal{L}\tilde u=\tilde f\quad\text{on $(0,L)$,}
    \quad\text{with $\tilde u(0)=\tilde\gamma_0$ 
and $\tilde u(L)=\tilde\gamma_L$.}
\]
If we denote the changes in the solution and the data by
\[
\delta u=\tilde u-u,\quad\delta f=\tilde f-f,\quad
\delta\gamma_0=\tilde\gamma_0-\gamma_0,\quad
\delta\gamma_L=\tilde\gamma_L-\gamma_L,
\]
then we need to show that $\delta u$ is small whenever $\delta f$, 
$\delta\gamma_0$~and $\delta\gamma_L$ are all small.

\begin{theorem}
If $c\ge0$ on~$(0,L)$, then \eqref{eq: Lu=f Dirichlet} has a unique 
solution~$u$.  This solution is continuous on~$[0,L]$ and is $C^2$ on~$(0,L)$.
Moreover, when the problem is perturbed as described above, 
\[
\|\delta u\|_\infty\le\max\{|\delta\gamma_0|,|\delta\gamma_L|\}
    +\cosh(\mu L/2)\|\delta f\|_\infty.
\]
\end{theorem}
\begin{proof}
We will not prove the hard part, namely existence.  Since the $\mathcal{L}$ is 
linear, the proof of uniqueness follows easily from 
Theorem~\ref{thm: Lu=f apriori infty}.  In fact, suppose that $u_1$~and $u_2$ 
are solutions, that is,
\[
\mathcal{L}u_1=f=\mathcal{L}u_2\quad\text{on $(0,L)$,}
    \quad\text{with $u_1(0)=\gamma_0=u_2(0)$ and $u_1(L)=\gamma_L=u_2(L)$.}
\]
The difference $v=u_1-u_2$ satisfies
\[
\mathcal{L}v=\mathcal{L}(u_1-u_2)=\mathcal{L}u_1-\mathcal{L}u_2=f-f=0
    \quad\text{on $(0,L)$},
\]
with $v(0)=u_1(0)-u_2(0)=\gamma_0-\gamma_0=0$ and
$v(L)=u_1(L)-u_2(L)=\gamma_L-\gamma_L=0$, so $\|v\|_\infty\le0$ by 
Theorem~\ref{thm: Lu=f apriori infty}, which means that $u_1=u_2$ on~$[0,L]$.

Similarly, since $\mathcal{L}$ is linear, we find that
\[
\mathcal{L}\delta u=\delta f\quad\text{on $(0,L)$,}
    \quad\text{with $\delta u(0)=\delta\gamma_0$ 
and $\delta u(L)=\delta\gamma_L$,}
\]
and Theorem~\ref{thm: Lu=f apriori infty} implies the desired estimate 
for~$\|\delta u\|_\infty$.
\end{proof}

\section{Discrete maximum principle}

We will now establish a maximum principle for our finite difference 
approximation to the two-point boundary-value problem~\eqref{eq: two-point bvp} 
in the case
\begin{equation}\label{eq: discrete max assumptions}
b(x)=0,\qquad\alpha_0=0,\quad\beta_0=1,\quad\alpha_L=0,\quad\beta_L=1.
\end{equation}
Thus, 
\begin{equation}\label{eq: simple Lu=f}
(\mathcal{L}u)(x)=-a(x)u''(x)+c(x)u(x)=f(x)\quad\text{for~$0<x<L$,}\quad
\text{with $u(0)=\gamma_0$~and $u(L)=\gamma_L$.}
\end{equation}
Likewise,
\[
(\mathcal{L}_{\Delta x}U)_p=-a_p\,\frac{U_{p+1}-2U_p+U_{p-1}}{\Delta x^2}
	+c_pU_p\quad\text{for $1\le p\le P-1$,}
\]
and our finite difference method is (case~1 in 
section~\ref{sec: gen two-point bvp})
\begin{equation}\label{eq: finite diff Dirichlet 1d}
(\mathcal{L}_{\Delta x}U)_p=f_p\quad\text{for $1\le p\le P-1$,}\quad
	\text{with $U_0=\gamma_0$ and $U_P=\gamma_L$.}
\end{equation}
The following discrete analogue of Theorem~\ref{lem: Lu<0} holds.

\begin{lemma}\label{lem: discrete LU<0}
Assume \eqref{eq: discrete max assumptions}.
If $c_p\ge0$ and $(\mathcal{L}_{\Delta x}U)_p<0$ for~$1\le p\le P-1$, then
the is no $p^*$ in the range $1\le p^*\le P-1$ for which
\[
U_{p^*}\ge 0,\qquad U_{p^*-1}\le U_{p^*}\qquad\text{and}\qquad
U_{p^*+1}\le U_{p^*}.
\]
\end{lemma}
\begin{proof}
If such a $p^*$ exists, then $U_{p^*+1}+U_{p^*-1}\le 2U_{p^*}$ so
\[
(\mathcal{L}_{\Delta x}U)_{p^*}
	=a_{p^*}\,\frac{-U_{p^*+1}+2U_{p^*}-U_{p^*-1}}{\Delta x^2}
	+c_{p^*}U_{p^*}\ge 0,
\]
contradicting the second hypothesis of the lemma.
\end{proof}

A discrete analogue of Theorem~\ref{thm: max principle 1d} then follows.

\begin{theorem}\label{thm: discrete max principle}
Assume \eqref{eq: discrete max assumptions}.
If $c_p\ge0$ and $(\mathcal{L}_{\Delta x}U)_p\le0$ for $1\le p\le P-1$, then 
\[
U_p\le\max\{U_0^+,U_P^+\}\quad\text{for $1\le p\le P-1$.}
\]
\end{theorem}
\begin{proof}
Let $\epsilon>0$~and $\mu>0$, and put $W_p=U_p+\epsilon e^{\mu x_p}$.  
We see from~\eqref{eq: second diff remainder} that the 
function~$g(x)=e^{\mu x}$ satisfies
\[
\frac{g(x_{p+1})-2g(x_p)+g(x_{p-1})}{\Delta x^2}-g''(x_p)
	=\frac{1}{\Delta x^2}\bigl[(R_3g)(x_p,\Delta x)+(R_3g)(x_p,-\Delta x)\bigr],
\]
where
\begin{align*}
(R_3g)(x_p,\Delta x)
	&=\frac{1}{4!}\int_{x_p}^{x_{p+1}}(x_{p+1}-y)^3g^{(4)}(y)\,dy,\\
(R_3g)(x_p,-\Delta x)
	&=\frac{(-1)^4}{4!}\int_{x_{p-1}}^{x_p}(y-x_{p-1})^3g^{(4)}(y)\,dy.
\end{align*}
Since $g^{(4)}(y)=\mu^4e^{\mu y}\ge0$ for all~$y$, it follows that
$(R_3g)(x_p,\pm\Delta x)\ge0$ and thus
\[
\frac{g(x_{p+1})-2g(x_p)+g(x_{p-1})}{\Delta x^2}\ge g''(x_p)=\mu^2e^{\mu x_p}.
\]
Hence, 
\begin{align*}
(\mathcal{L}_{\Delta x}W)_p&=(\mathcal{L}_{\Delta x}U)_p
	+\epsilon\biggl(-a_p\,\frac{g(x_{p+1})-2g(x_p)+g(x_{p-1})}{\Delta x^2}
	+c_pg(x_p)\biggr)\\
	&\le (\mathcal{L}_{\Delta x}U)_p
		+\epsilon\bigl(-a_p\mu^2e^{\mu x_p}+c_pe^{\mu x_p}\bigr)
	\le0-\epsilon(a_{\min}\mu^2-\|c\|_\infty)e^{\mu x_p}, 
\end{align*}
and by choosing $\mu^2>\|c\|_\infty/a_{\min}$ we can ensure that 
$(\mathcal{L}_{\Delta x}U)_p<0$ for~$1\le p\le P-1$.  By 
Lemma~\ref{lem: discrete LU<0}, we conclude that
\[
U_p\le W_p\le W_p^+\le\max\{W_0^+,W_P^+\}
	=\max\{U_0^++\epsilon,U_P^++\epsilon e^{\mu L}\}
\]
for $1\le p\le P-1$.  Since this inequality holds for any $\epsilon>0$, the 
result follows.
\end{proof}

Next is a discrete version of Theorem~\ref{thm: max min 1d}.

\begin{theorem}
Assume \eqref{eq: discrete max assumptions}.
If $c_p\ge0$ and $(\mathcal{L}_{\Delta x}U)_p=f_p$ for $1\le p\le L$, then
\[
\max_{0\le p\le P}U_p\le\max\{U^+_0,U^+_P\}+\frac{L^2}{8a_{\min}}
	\max_{1\le q\le P-1}f_q^+
\]
and
\[
\min_{0\le p\le P}U_p\ge\min\{U^+_0,U^+_P\}+\frac{L^2}{8a_{\min}}
	\max_{1\le q\le P-1}f_q^-
\]
\end{theorem}
\begin{proof}
Let 
\[
W_p=\max\{U^+_0,U^+_P\}+V_p\max_{1\le q\le P-1}f_q^+
\quad\text{where}\quad
V_p=\frac{x_p(L-x_p)}{2a_{\min}}.
\]
Since the second-order central difference formula is exact for a quadratic 
polynomial,
\[
\frac{V_{p+1}-2V_p+V_{p-1}}{\Delta x^2}=\frac{-1}{a_{\min}}
\]
and thus, noting that $V_p\ge0$, we conclude that
\[
(\mathcal{L}_{\Delta x}V)_p=\frac{a_p}{a_{\min}}+c_pV_p\ge1
	\quad\text{for $1\le p\le P-1$.}
\]
It follows that
\[
(\mathcal{L}_{\Delta x}W)_p=\max\{U^+_0,U^+_P\}(\mathcal{L}_{\Delta x}1)_p
	+\Bigl(\max_{1\le q\le P-1}f_q^+\Bigr)(\mathcal{L}_{\Delta x}V)_p
	\ge\max_{1\le q\le P-1}f_q^+
\]
and so
\[
\bigl(\mathcal{L}_{\Delta x}(U-W)\bigr)_p=f_p-(\mathcal{L}_{\Delta x}W)_p
	\le f_p-\max_{1\le q\le P-1}f_q^+\le0\quad\text{for $1\le p\le P-1$,}
\]
with
\[
(U-W)_0=U_0-\max{U_0^+,U_P^+}\le0
\quad\text{and}\quad
(U-W)_P=U_P-\max{U_0^+,U_P^+}\le0.
\]
By Theorem~\ref{thm: discrete max principle},
\[
U_p-W_p\le\max\{(U-W)_0^+,(U-W)_P^+\}\le0\quad\text{for $1\le p\le P-1$,}
\]
and the first inequality follows because
\[
U_p\le W_p\le\max\{U^+_0,U^+_P\}
	+\biggl(\max_{0\le x\le L}\frac{x(L-x)}{2a_{\min}}\biggr)
	\biggl(\max_{1\le q\le P-1}f_q^+\biggr). 
\]
The second follows because $\bigl(\mathcal{L}_{\Delta x}(-U)\bigr)_p=-f_p$,
$(-U)_p^+=-U_p^-$~and $(-f)_p^+=-f_p^-$.
\end{proof}

Our final result for this section is a discrete version of 
Theorem~\ref{thm: Lu=f apriori infty}.

\begin{theorem}\label{thm: discrete apriori 1D}
Assume \eqref{eq: discrete max assumptions}. If $c\ge0$ on~$(0,L)$, then the
finite difference equations~\eqref{eq: finite diff Dirichlet 1d} have a unique
solution~$U_p$, and
\[
\max_{0\le p\le P}|U_p|\le\max\{|\gamma_0|,|\gamma_L|\}
    +\frac{L^2}{8a_{\min}}\max_{1\le q\le P-1}|f_q|.
\]
\end{theorem}
\begin{proof}
The \emph{a priori} estimate for~$U_p$ follows at once from 
Theorem~\ref{thm: max min 1d}.  To prove existence and uniqueness, we write the 
finite difference equations in matrix form, as 
in~\eqref{eq: finite diff matrix 1d} except that 
now $\boldsymbol{B}=\boldsymbol{0}$ so
\[
(\boldsymbol{A}+\boldsymbol{C})\boldsymbol{U}=\boldsymbol{f}+\boldsymbol{g}.
\]
The \emph{a priori} estimate shows that if $f_p=0$ for~$1\le p\le P-1$ and 
$\gamma_0=0=\gamma_L$, then $U_p=0$ for~$0\le p\le P$.  In other words if 
$\boldsymbol{f}=\boldsymbol{0}=\boldsymbol{g}$, then 
$\boldsymbol{U}=\boldsymbol{0}$.  Thus, the homogeneous linear system admits 
only the trivial solution, which implies that the 
matrix~$\boldsymbol{A}+\boldsymbol{C}$ is non-singular and so the linear system 
is uniquely solvable for any $f_p$, $\gamma_0$~and $\gamma_L$.
\end{proof}

\section{An error bound}

The finite difference method defined in section~\ref{sec: gen two-point bvp}
is said to be \emph{stable} if a unique solution~$U_p$ exists for any choice of 
the data $f_p$, $\gamma_0$~and $\gamma_L$, and if there is a constant~$C$ --- 
independent of $f_p$, $\gamma_0$, $\gamma_L$~and $\Delta x$ --- such that
\[
\max_{0\le p\le P}|U_p|\le C\Bigl(|\gamma_0|+|\gamma_L|
    +\max_{1\le p\le P-1} |f_p|\Bigr).
\]
For example, by Theorem~\ref{thm: discrete apriori 1D}, the 
conditions~\eqref{eq: discrete max assumptions} are sufficient to ensure 
stability.

Suppose $\alpha_0=0=\alpha_L$ (that is, case~1). We define the 
\emph{local truncation error}~$\tau_p$ by
\[
\tau_p=f_p-(\mathcal{L}_{\Delta x}u)_p
    =(\mathcal{L}u)_p-(\mathcal{L}_{\Delta x}u)_p,
\]
and note that
\begin{multline*}
\tau_p=-a_p\biggl(
    u''(x_p)-\frac{u(x_p+\Delta x)-2u(x_p)+u(x_p-\Delta x)}{\Delta x^2}\biggr)\\
+b_p\biggl(u'(x_p)-\frac{u(x_p+\Delta x)-u(x_p-\Delta x)}{2\,\Delta x}\biggr)
\end{multline*}
so, by Theorems \ref{thm: 2nd central diff}~and \ref{thm: first central diff},
\begin{equation}\label{eq: tau_p bound}
|\tau_p|\le\biggl(\frac{|a_p|}{12}\,\|u^{(4)}\|_\infty
    +\frac{|b_p|}{6}\,\|u^{(3)}\|_\infty\biggr)\,\Delta x^2.
\end{equation}
Since $(\mathcal{L}_{\Delta x}U)_p=f_p=(\mathcal{L}u)_p$ and since the 
finite difference operator~$\mathcal{L}_{\Delta x}$ is linear, it follows that
the \emph{solution error},
\[
E_p=U_p-u(x_p)
\]
satisfies
\[
(\mathcal{L}_{\Delta x}E)_p=(\mathcal{L}_{\Delta x}U)_p
    -(\mathcal{L}_{\Delta x}u)_p=\tau_p\quad\text{for $1\le p\le P-1$.}
\]
Moreover, since $U_0=\gamma_0=u(x_0)$~and $U_P=\gamma_L=u(x_P)$ we have 
$E_0=0=E_P$.  Therefore, the stability property applies with $U_p$, $f_p$, 
$\gamma_0$~and $\gamma_L$ replaced by $E_p$, $\tau_p$, $0$~and $0$, 
respectively, and so
\[
\max_{0\le p\le P}|E_p|\le C\max_{1\le p\le P-1}|\tau_p|.
\]
Combining this estimate with~\eqref{eq: tau_p bound} we obtain the error bound
\[
|U_p-u(x_p)|\le C \biggl(\frac{\|a\|_\infty}{12}\,\|u^{(4)}\|_\infty
    +\frac{\|b\|_\infty}{6}\,\|u^{(3)}\|_\infty\biggr)\,\Delta x^2
    \quad\text{for $0\le p\le P$.}
\]
In other words,
\[
U_p=u(x_p)+O(\Delta x^2)\quad\text{as $\Delta x\to0$, for $0\le p\le P$,}
\]
showing that the finite difference method is \emph{second-order accurate}.

It turns out that for many numerical methods, there is an exponent~$r>0$ and a 
continuous function~$v(x)$ such that
\begin{equation}\label{eq: Ep asymp}
E_p=U_p-u(x_p)=v(x_p)\,\Delta x^r+O(\Delta x^{r+\epsilon})
\end{equation}
for some~$\epsilon>0$.  Therefore, if we let
\[
\mathcal{E}(\Delta x)=\max_{0\le p\le P}|E_p|
\]
then
\[
\mathcal{E}(\Delta x)\approx\Bigl(\max_{0\le x\le L}|v(x)|\Bigr)\,\Delta x^2.
\]
It follows that we can estimate the value of~$r$ by computing the ratio
\[
\frac{\mathcal{E}(2\,\Delta x)}{\mathcal{E}(\Delta x)}\approx 2^r
\]
and taking logarithms (to base~2):
\[
r\approx\log_2\frac{\mathcal{E}(2\,\Delta x)}{\mathcal{E}(\Delta x)}.
\]
More precisely, the logarithm on the right will converge to~$r$ 
as~$\Delta x\to0$.

\begin{table}
\caption{Convergence behaviour of the finite difference solution from
\cref{example: bvp1d conv}}\label{table: bvp1d conv}
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{r|cc}
\multicolumn{1}{c|}{$P$}&max error&rate\\
\hline
   8&   5.28e-03&\\
  16&   1.33e-03&   1.986\\
  32&   3.34e-04&   1.998\\
  64&   8.35e-05&   1.999\\
 128&   2.09e-05&   2.000
\end{tabular}
\end{center}
\end{table}

\begin{example}\label{example: bvp1d conv}
Let $e_P=\max_{0\le p\le P}|U_p-u(x_p)|$ denote the maximum error in the finite 
difference solution for a grid with $P$~subintervals.  The argument above shows 
that if the error behaves as in~\eqref{eq: Ep asymp} then
\begin{equation}\label{eq: eP asymp}
r\approx\log_2\frac{E_{P/2}}{E_P}.
\end{equation}
\Cref{table: bvp1d conv} shows the values of the maximum error~$e_P$ and 
the estimated convergence rates according to~\eqref{eq: eP asymp} as we 
repeatedly double~$P$, using the problem from \cref{example: bvp1d example}.
These results confirm that the finite difference scheme is second-order 
accurate, that is, $r=2$.
\end{example}

\begin{Exercises}

\exercise\label{ex: u const f}
Verify that \eqref{eq: u const f} satisfies \eqref{eq: model 1d} if $f(x)=c$.

\exercise\label{ex: variation of params}
By following the steps below, use variation of parameters to  verify that 
\eqref{eq: model 1d exact soln} solves \eqref{eq: model 1d} for a 
general~$f(x)$.  Let
\[
u_1(x)=L-x\quad\text{and}\quad u_2(x)=x
\]
and write
\[
u(x)=v_1(x)u_1(x)+v_2(x)u_2(x).
\]
\begin{description}
\item{(i)} Verify that $u_1$~and $u_2$ are solutions of the homogeneous 
equation~$u''=0$, and find their Wronskian
\[
W=\begin{vmatrix}u_1&u_2\\ u_1'&u_2' \end{vmatrix}.
\]
\item{(ii)} Hence, noting that $u''=-f(x)$, determine
\[
v_1'(x)=-\frac{u_2(x)[-f(x)]}{W(x)}=\frac{u_2(x)f(x)}{W(x)}
\quad\text{and}\quad
v_2'(x)=\frac{u_1(x)[-f(x)]}{W(x)}=-\frac{u_1(x)f(x)}{W(x)}.
\]
\item{(iii)}
Find $v_1$~and $v_2$, using the boundary conditions $u(0)=\gamma_0$~and 
$u(L)=\gamma_L$ to determine the constants of integration.
\item{(iv)} Deduce from \eqref{eq: model 1d exact soln} that if $f(x)\ge0$ 
for~$0\le x\le L$, then the graph of~$u(x)$ is always above the straight line 
joining $(0,\gamma_0)$~and $(L,\gamma_L)$.  What if $f(x)\le0$ 
for~$0\le x\le L$?
\end{description}
\begin{ans}
(i) $W(x)=L$\quad (ii) $v_1'=\dfrac{xf(x)}{L}$, 
$v_2'=\dfrac{(L-x)f(x)}{L}$\quad (iii) We have
\[
v_1(x)=A+\int_0^x\frac{yf(y)}{L}\,dy
\quad\text{and}\quad
v_2(x)=B+\int_x^L\frac{(L-y)f(y)}{L}\,dy,
\]
and the boundary conditions imply that $A=\gamma_0$ and $B=\gamma_L$.
\end{ans}

\exercise
Use Taylor expansion to find the coefficient~$c$ such that, as~$x\to0$,
\begin{description}
\item{(i)}
$\dfrac{x}{1-x^2}-\sin x=cx^3+O(x^5)$.
\item{(ii)}
$\log\cos x=cx^2+O(x^4)$.
\item{(iii)}
$\dfrac{x-\sinh x}{x^3}=c+O(x^2)$.
\end{description}
\begin{ans}
(i) $5/6$\quad (ii) $-1/2$\quad (iii) $-1/6$
\end{ans}

\exercise
Use Taylor expansion to find the coefficient~$c$ such that
\begin{description}
\item{(i)} $\dfrac{3u(x)-4u(x-\Delta x)+u(x-2\Delta x)}{2\Delta x}
=u'(x)+cu'''(x)\,\Delta x^2+O(\Delta x^3)$.
\item{(ii)} $\dfrac{-u(x+2\Delta x)+4u(x+\Delta x)-3u(x)}{2\Delta x}
=u'(x)+cu'''(x)\,\Delta x^2+O(\Delta x^3)$.
\end{description}
\begin{ans}
(i) $c=-1/3$ \quad(ii) $c=-1/3$
\end{ans}

\begin{table}
\caption{Errors in the approximations of $f'(a)$ from 
Exercise~\ref{ex: complex deriv}.}
\label{tab: complex deriv}
\begin{center}
\renewcommand{\arraystretch}{1.2}
\ttfamily
\begin{tabular}{lrr}
\multicolumn{1}{c}{$h$}&
\multicolumn{1}{c}{$\epsilon_1(h)$}&
\multicolumn{1}{c}{$\epsilon_2(h)$}\\
\hline
$10^{-2}$ &-9.00e-06&9.01e-06\\
$10^{-4}$ &-9.02e-10&9.01e-10\\
$10^{-6}$ & 8.52e-11&9.02e-14\\
$10^{-8}$ &-4.02e-09&0.00e+00\\
$10^{-10}$& 1.22e-06&0.00e+00
\end{tabular}
\end{center}
\end{table}

\exercise\label{ex: complex deriv} 
Let $f$ be complex analytic in a neighbourhood of a point~$a$ on the real axis, 
and assume that $f(x)$ is real when $x$ is real.
\begin{description}
\item{(i)} Show that $\Im f(a+ih)/h=f'(0)+O(h^2)$ if $h\to0$ and $h$ is real.
\item{(ii)} \cref{tab: complex deriv} shows the values of
\[
\epsilon_1(h)=\frac{f(a+h)-f(a-h)}{2h}-f'(0)
\quad\text{and}\quad
\epsilon_2(h)=\frac{\Im f(a+ih)}{h}-f'(0),
\]
when $f(x)=x^2+\sin x$ and $a=1$, for different choices of~$h$, with all 
computations performed in standard 64~bit floating-point arithmetic.  Explain 
the different behavior of $\epsilon_1(h)$ and $\epsilon_2(h)$.
\end{description}

\exercise
Consider the boundary value problem
\[
\begin{aligned}
-u''+2u'-u&=1&&\text{for $0<x<2$,}\\
u&=1&&\text{at $x=0$,}\\
u&=-1&&\text{at $x=2$.}
\end{aligned}
\]
\begin{description}
\item{(i)}
Set up a finite difference approximation as explained in lectures
with $P=4$ (so $\Delta x=1/2$).
\item{(ii)}
Eliminate the variables $U_0$ and $U_4$ to obtain a $3\times3$ linear
system.
\item{(iii)}
Solve the linear system to find $U_1$, $U_2$~and $U_3$.  (You can use
for favourite software.)
\item{(iv)}
Find the exact solution~$u(x)$, and hence compute the errors~$E_p=U_p-u(x_p)$ 
in your numerical solution.
\end{description}
\begin{ans}
(i) $-6U_{p-1}+7U_p-2U_{p-1}=1$ for $1\le p\le3$\quad
(ii) Since $U_0=1$ and $U_4=-1$,
\[
\begin{bmatrix}7&-2& 0\\ -6& 7&-2\\  0&-6& 7\end{bmatrix}
\begin{bmatrix}U_1\\ U_2\\ U_3\end{bmatrix}
=\begin{bmatrix}7\\ 1\\ -1\end{bmatrix}
\]
(iii) $U_1=269/175=1.5371$, $U_2=47/25=1.88$,
$U_3=257/175=1.4686$\\
(iv) $u=(2-x)e^x-1$ so the errors $E_p=U_p-u(x_p)$ are
$E_1=0.06406$, $E_2=-0.16172$, $E_3=0.22773$.
\end{ans}

\exercise
Consider the two-point boundary-value problem
\[
-u''=f(x)\quad\text{for $0<x<L$,}
	\quad\text{with $-u'(0)=\gamma_0$ and $u'(L)=\gamma_L$.}
\]
\begin{description}
\item{(i)} Show that if $u(x)$ is a solution then so is $u(x)+C$ for any 
constant~$C$.
\item{(ii)} Show that if $u(x)$ exists, then 
$\gamma_0+\gamma_L+\int_0^Lf(x)\,dx=0$.
\item{(iii)} Use central difference approximations of the form 
\eqref{eq: u'' approx}~and \eqref{eq: u' approx} to derive a $(P+1)\times(P+1)$ 
linear system 
$\boldsymbol{A}\boldsymbol{U}=\boldsymbol{f}+\boldsymbol{g}$ that yields
$U_p\approx u(x_p)$ for~$0\le p\le P$.  When $P=5$ you should obtain
\[
\frac{1}{\Delta x^2}\begin{bmatrix}
 1&-1&  &  &  &\\                     
-1& 2&-1&  &  &\\
  &-1& 2&-1&  &\\
  &  &-1& 2&-1&\\
  &  &  &-1& 2&-1\\
  &  &  &  &-1& 1
\end{bmatrix}
\begin{bmatrix}U_0\\ U_1\\ U_2\\ U_3\\ U_4\\ U_5\end{bmatrix}
=\begin{bmatrix}\tfrac12f_0\\ f_1\\ f_2\\ f_3\\ f_4\\ \tfrac12 f_5
\end{bmatrix}
+\frac{1}{\Delta x}
\begin{bmatrix}\gamma_0\\ 0\\ 0\\ 0\\ 0\\ \gamma_L\end{bmatrix}.
\]
Hint: applying central differences at $x_0$~and $x_P$ requires the use of 
``ghost'' grid points $x_{-1}$~and $x_{P+1}$ lying outside the interval~$[0,L]$.
However, the boundary conditions allow you to eliminate $U_{-1}$~and $U_{P+1}$.
\item{(iv)} Show that if $\boldsymbol{U}=[U_p]$ is a solution, then so is 
$[U_p+C]$.
\item{(v)} Show that if a solution~$\boldsymbol{U}$ exists, then
\[
\gamma_0+\gamma_L+\bigl(\tfrac12f_0+f_1+f_2+\cdots+f_{P-1}+\tfrac12f_P\bigr)
\Delta x=0.
\]
\item{(vi)} If we interpret the ODE as a steady-state heat equation, what is 
the physical meaning of the condition in~(ii), and of its discrete analogue 
in~(v)?
\end{description}

\exercise
How can we use the $LDL^\top$ factorization of an $n\times n$, symmetric 
tridiagonal matrix~$\boldsymbol{A}$ to compute its determinant?
\begin{ans} 
$\det\boldsymbol{A}=d_1d_2\cdots d_n$
\end{ans}

\exercise
Find the $LDL^\top$ factorization of the matrix
\[
\boldsymbol{A}=\begin{bmatrix}1&-1&0&0\\ -1&4&6&0\\ 0&6&14&6\\ 0&0&6&22
\end{bmatrix}.
\]
\begin{ans}
$\boldsymbol{L}=\begin{bmatrix}1&0&0&0\\ -1&1&0&0\\ 0&2&1&0\\ 0&0&3&1 
\end{bmatrix}$,
$\boldsymbol{D}=\begin{bmatrix}1&&&\\ &3&&\\ &&2&&\\ &&&4\end{bmatrix}$
\end{ans}

\exercise
If $\boldsymbol{A}$ is symmetric tridiagonal matrix, then instead of the 
factorization~\eqref{eq: L D LT} we can seek a tridiagonal, upper triangular
matrix~$\boldsymbol{R}$ such that 
$\boldsymbol{A}=\boldsymbol{R}^\top\boldsymbol{R}$.  We call $\boldsymbol{R}$ 
the \emph{Cholesky factor} of~$\boldsymbol{A}$.
\begin{description}
\item{(i)} Show that if a matrix~$\boldsymbol{A}$ has a Cholesky factorization, 
then $\boldsymbol{A}$ is necessarily symmetric and positive semidefinite.
\item{(ii)} Show that if the Cholesky factor is non-singular, then 
$\boldsymbol{A}$ must be \emph{strictly} positive-definite.
\item{(iii)} Consider the $5\times5$ case and denote the entries 
of~$\boldsymbol{A}$ as in~\eqref{eq: A symm tridiagonal}.  Determine a sequence 
of formulae to compute the entries of
\[
\boldsymbol{R}=\begin{bmatrix}
d_1&u_1&   &   &\\
   &d_2&u_2&   &\\
   &   &d_3&u_3&\\
   &   &   &d_4&u_4\\
   &   &   &   &u_5\end{bmatrix}.
\]
\item{(iv)} Hence formulate an algorithm to compute~$\boldsymbol{R}$ in the 
general, $n\times n$~case.
\item{(v)} Formulate an in-place version of the algorithm in part~(iv).
\item{(vi)} How is the Cholesky factorization used to solve a linear 
system~$\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b}$?
\item{(vii)} Show that the algorithm is stable (provided $\boldsymbol{A}$ is 
positive-definite) in the sense that all entries of~$\boldsymbol{R}$ can be 
bounded by entries of~$\boldsymbol{A}$.
\end{description}
\begin{ans}
(i)
\[
\begin{aligned}
d_1&=\sqrt{\alpha_1},&u_1&=\beta_1/d_1,\\
d_2&=\sqrt{\alpha_2-u_1^2},&u_2&=\beta_2/d_2\\
d_3&=\sqrt{\alpha_3-u_2^2},&u_3&=\beta_3/d_3\\
d_4&=\sqrt{\alpha_4-u_3^2},&u_4&=\beta_3/d_3\\
d_5&=\sqrt{\alpha_5-u_4^2}.
\end{aligned}
\]
\end{ans}


\exercise
Consider the ODE in \emph{divergence form}:
\begin{equation}\label{eq: ODE divergence form}
-\frac{d}{dx}\biggl(a(x)\,\frac{du}{dx}\biggr)=f(x)\quad\text{for $0<x<L$.}
\end{equation}
\begin{description}
\item{(i)}
Find $w(x)$ such that
\begin{multline*}
\frac{1}{\Delta x}\biggl(
 a(x+\tfrac12\Delta x)\,\frac{u(x+\Delta x)-u(x)}{\Delta x}
-a(x-\tfrac12\Delta x)\,\frac{u(x)-u(x-\Delta x)}{\Delta x}\\
    =\frac{d}{dx}\biggl(a(x)\,\frac{du}{dx}\biggr)+w(x)\Delta x^2+O(\Delta x^4)
    \quad\text{as $\Delta x\to0$.}
\end{multline*}
Hint: for $h=\tfrac12\Delta x$, let 
\[
v(x)=a(x)\,\frac{u(x+h)-u(x-h)}{2h}
\]
and use the result
\[
\frac{v(x+h)-v(x-h)}{2h}=v'(x)+\frac{1}{6}\,v'''(x)h^2+O(h^4)
    \quad\text{as $h\to0$.}
\]
\item{(ii)} Hence devise a finite difference scheme with $\Delta x=L/P$ to 
solve \eqref{eq: ODE divergence form} subject to the Dirichlet boundary 
conditions $u(0)=\gamma_0$~and $u(L)=\gamma_L$.
\item{(iii)} Write out the linear system in matrix form when~$P=5$.
\end{description}
\begin{ans}
(i) $w(x)=\bigl[(au')'''(x)+(au''')'(x)\bigr]/24$\quad (ii) Putting
$a_{p\pm1/2}=a(x_p\pm\tfrac12\Delta x)$ we have
\[
-\frac{1}{\Delta x}\biggl(a_{p+1/2}\,\frac{U_{p+1}-U_p}{\Delta x}
    -a_{p-1/2}\,\frac{U_p-U_{p-1}}{\Delta x}\biggr)=f_p
    \quad\text{for $1\le p\le P-1$,}
\]
with $U_0=\gamma_0$~and $U_P=\gamma_L$.\quad (iii)
\begin{multline*}
\frac{1}{\Delta x^2}\begin{bmatrix}
            (a_{1/2}+a_{3/2})&-a_{3/2}&&\\
   -a_{3/2}&(a_{3/2}+a_{5/2})&-a_{5/2}&\\
  &-a_{5/2}&(a_{5/2}+a_{7/2})&-a_{7/2}\\
 &&-a_{7/2}&(a_{7/2}+a_{9/2})
\end{bmatrix}
\begin{bmatrix}U_1\\ U_2\\ U_3\\ U_4\end{bmatrix}\\
=\begin{bmatrix}f_1\\ f_2\\ f_3\\ f_4 \end{bmatrix}+\frac{1}{\Delta x^2}
\begin{bmatrix}a_{1/2}\gamma_0\\ \\ \\ a_{9/2}\gamma_L \end{bmatrix}.
\end{multline*}
\end{ans}

\end{Exercises}
